{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.models import Model\n",
    "\n",
    "from machine_learning.encoders import WaveNet_Encoder, CNN_Encoder, CNN_LSTM_Encoder, LSTM_Encoder, WaveNetOpt_Encoder\n",
    "from machine_learning.decoders import CNN_Decoder, WaveNet_Decoder, CNN_LSTM_Decoder, LSTM_Decoder\n",
    "from machine_learning.anomaly_detection import AutoEncoder, ED_Feedback, AutoEncoder2"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "# Versuchsparameter\n",
    "normal = 0\n",
    "anomalies_percentage = 2\n",
    "dataset = 'energy'\n",
    "model_type = 'cnn'\n",
    "\n",
    "# ML-Parameter\n",
    "batch_size = 32\n",
    "\n",
    "# automatisch generiert\n",
    "classes = [0,3] #list(range(num_anomaly+1))\n",
    "percentage = anomalies_percentage/100\n",
    "model_str = f'{dataset}-c{\"\".join(str(c)+\"-\" for c in classes)}p{anomalies_percentage}.hdf5'\n",
    "\n",
    "model_dict = {\n",
    "    'cnn': (CNN_Encoder, CNN_Decoder),\n",
    "    'wavenet': (WaveNet_Encoder, WaveNet_Decoder),\n",
    "    'wavenet-opt': (WaveNetOpt_Encoder, WaveNet_Decoder),\n",
    "    'lstm': (LSTM_Encoder, LSTM_Decoder),\n",
    "    'cnn-lstm': (CNN_LSTM_Encoder, CNN_LSTM_Decoder),\n",
    "}\n",
    "\n",
    "# get the working directory of the ipykernel\n",
    "working_dir = os.getcwd()\n",
    "\n",
    "# define subdirectories\n",
    "data_path = os.path.join(working_dir, \"data\")\n",
    "dataset_path = os.path.join(data_path, dataset)\n",
    "trainingset_path = os.path.join(dataset_path, \"train\")\n",
    "testset_path = os.path.join(dataset_path, \"test\")\n",
    "\n",
    "models_path = os.path.join(working_dir, \"saved_models\")\n",
    "output_path = os.path.join(models_path, model_type)\n",
    "Path(output_path).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(model_str)\n",
    "print(trainingset_path)\n",
    "print(output_path)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "energy-c0-3-p2.hdf5\n",
      "/Users/david/repos/deep-anomaly-detection/data/energy/train\n",
      "/Users/david/repos/deep-anomaly-detection/saved_models/wavenet-opt\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "def aursad(dataset='train', classes=classes):\n",
    "\n",
    "    path = None\n",
    "    if dataset.lower() == 'train':\n",
    "        path = trainingset_path\n",
    "    elif dataset.lower() == 'test':\n",
    "        path = testset_path\n",
    "    else:\n",
    "        raise ValueError(\"Invalid dataset specified\")\n",
    "    \n",
    "    x = np.load(os.path.join(path, \"x.npy\"))\n",
    "    y = np.load(os.path.join(path, \"y.npy\"))\n",
    "    \n",
    "    # extract the relevant classes only\n",
    "    mask = [i in classes for i in y]\n",
    "    x = x[mask,:,:]\n",
    "    y = y[mask]\n",
    "    \n",
    "    return x, y\n",
    "\n",
    "def unsupervised(percentage=percentage, batch_size=batch_size):\n",
    "    \n",
    "    x,y = aursad()\n",
    "        \n",
    "    num_normal_samples = np.sum(y == normal)\n",
    "    num_anomalies = int(num_normal_samples / (1-percentage) * percentage)\n",
    "\n",
    "    # extracts the first anomalous occurencies, until the desired percentage is achieved\n",
    "    anomaly_idx = y != normal\n",
    "    cum_anomaly_idx = np.cumsum(anomaly_idx)\n",
    "    anomaly_mask = cum_anomaly_idx <= num_anomalies\n",
    "\n",
    "    mask = np.logical_or(y == normal, anomaly_mask)\n",
    "\n",
    "    x = x[mask,::]\n",
    "    y = y[mask]\n",
    "    \n",
    "    total = x.shape[0]\n",
    "    n = (total // batch_size) * batch_size\n",
    "    \n",
    "    x = x[:n,:,:]\n",
    "    y = y[:n]\n",
    "       \n",
    "    return x, y\n",
    "\n",
    "def semi_supervised(batch_size=batch_size):\n",
    "    \n",
    "    return unsupervised(percentage=0, batch_size=batch_size)\n",
    "\n",
    "def supervised(dataset = \"train\", batch_size = batch_size):\n",
    "    \n",
    "    x,y = aursad(dataset=dataset)\n",
    "    \n",
    "    total = x.shape[0]\n",
    "    n = (total // batch_size) * batch_size\n",
    "    \n",
    "    x = x[:n,:,:]\n",
    "    y = y[:n]\n",
    "       \n",
    "    return x, y\n",
    "\n",
    "def holdout_validation(x, y, percentage=0.8, batch_size=batch_size):\n",
    "\n",
    "    split_idx = int((len(y) * 0.8 // batch_size) * batch_size)\n",
    "\n",
    "    x_train = x[:split_idx,::]\n",
    "    y_train = y[:split_idx]\n",
    "    x_val = x[split_idx:,::]\n",
    "    y_val = y[split_idx:]\n",
    "\n",
    "    return (x_train, y_train), (x_val, y_val)\n",
    "\n",
    "\n",
    "x_peek, _= aursad()\n",
    "sequence_length = x_peek.shape[1]\n",
    "num_features = x_peek.shape[2]\n",
    "\n",
    "print(f'Sequence length = {sequence_length}')\n",
    "print(f'Number of features = {num_features}')\n",
    "\n",
    "print(len(unsupervised(percentage=percentage)[0]))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Sequence length = 480\n",
      "Number of features = 4\n",
      "1120\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "# load the training data\n",
    "train_x, _ = unsupervised()\n",
    "#latent_dim = 8\n",
    "\n",
    "def build(latent_dim, is_variational=False):\n",
    "    # build the model\n",
    "    vaeAD = AutoEncoder(encoder=model_dict[model_type][0], decoder=model_dict[model_type][1],\n",
    "                      input_shape=train_x.shape, latent_dim=latent_dim, is_variational=is_variational)\n",
    "\n",
    "    vae = vaeAD.get_model()\n",
    "    vae.summary()\n",
    "\n",
    "    return vae, vaeAD"
   ],
   "outputs": [],
   "metadata": {
    "scrolled": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "# train the VAE on MNIST digits\n",
    "x, y = unsupervised()\n",
    "(x_train, _), (x_val, _) = holdout_validation(x, y)\n",
    "print(x_train.shape)\n",
    "\n",
    "callbacks = [keras.callbacks.EarlyStopping(monitor='val_loss', patience=8)]\n",
    "\n",
    "\n",
    "all_losses = list()\n",
    "for l in range(1,17):\n",
    "    vae, vaeAD = build(latent_dim=6)\n",
    "    history = vae.fit(x_train, x_train, epochs=360, shuffle=True, batch_size=batch_size,\n",
    "        callbacks=callbacks, validation_data=(x_val,x_val))\n",
    "    loss = np.min(history.history[\"val_loss\"])\n",
    "    all_losses[l] = loss\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "(896, 480, 4)\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 480, 4)]          0         \n",
      "_________________________________________________________________\n",
      "wavenet-encoder (Functional) (None, 6)                 29862     \n",
      "_________________________________________________________________\n",
      "wavenet-decoder (WaveNet_Dec (None, 480, 4)            22620     \n",
      "=================================================================\n",
      "Total params: 52,482\n",
      "Trainable params: 52,482\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/360\n",
      "WARNING:tensorflow:AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x15f3bf9d0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_train_function.<locals>.train_function at 0x15f3bf9d0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "28/28 [==============================] - ETA: 0s - loss: 0.1719WARNING:tensorflow:AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x15f701670> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "WARNING: AutoGraph could not transform <function Model.make_test_function.<locals>.test_function at 0x15f701670> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: unsupported operand type(s) for -: 'NoneType' and 'int'\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "28/28 [==============================] - 1s 25ms/step - loss: 0.1708 - val_loss: 0.0852\n",
      "Epoch 2/360\n",
      "28/28 [==============================] - 1s 20ms/step - loss: 0.0735 - val_loss: 0.0430\n",
      "Epoch 3/360\n",
      "28/28 [==============================] - 1s 20ms/step - loss: 0.0357 - val_loss: 0.0245\n",
      "Epoch 4/360\n",
      "28/28 [==============================] - 1s 20ms/step - loss: 0.0242 - val_loss: 0.0233\n",
      "Epoch 5/360\n",
      "28/28 [==============================] - 1s 20ms/step - loss: 0.0231 - val_loss: 0.0229\n",
      "Epoch 6/360\n",
      "28/28 [==============================] - 1s 20ms/step - loss: 0.0231 - val_loss: 0.0227\n",
      "Epoch 7/360\n",
      "28/28 [==============================] - 1s 20ms/step - loss: 0.0230 - val_loss: 0.0225\n",
      "Epoch 8/360\n",
      "28/28 [==============================] - 1s 20ms/step - loss: 0.0225 - val_loss: 0.0224\n",
      "Epoch 9/360\n",
      "28/28 [==============================] - 1s 20ms/step - loss: 0.0222 - val_loss: 0.0222\n",
      "Epoch 10/360\n",
      "28/28 [==============================] - 1s 20ms/step - loss: 0.0222 - val_loss: 0.0221\n",
      "Epoch 11/360\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 0.0219 - val_loss: 0.0219\n",
      "Epoch 12/360\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 0.0219 - val_loss: 0.0218\n",
      "Epoch 13/360\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 0.0219 - val_loss: 0.0217\n",
      "Epoch 14/360\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 0.0217 - val_loss: 0.0215\n",
      "Epoch 15/360\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 0.0215 - val_loss: 0.0214\n",
      "Epoch 16/360\n",
      "28/28 [==============================] - 1s 20ms/step - loss: 0.0216 - val_loss: 0.0213\n",
      "Epoch 17/360\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 0.0213 - val_loss: 0.0212\n",
      "Epoch 18/360\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 0.0214 - val_loss: 0.0211\n",
      "Epoch 19/360\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 0.0213 - val_loss: 0.0208\n",
      "Epoch 20/360\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 0.0209 - val_loss: 0.0204\n",
      "Epoch 21/360\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 0.0205 - val_loss: 0.0203\n",
      "Epoch 22/360\n",
      "28/28 [==============================] - 1s 20ms/step - loss: 0.0201 - val_loss: 0.0199\n",
      "Epoch 23/360\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 0.0198 - val_loss: 0.0196\n",
      "Epoch 24/360\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 0.0195 - val_loss: 0.0191\n",
      "Epoch 25/360\n",
      "28/28 [==============================] - 1s 20ms/step - loss: 0.0191 - val_loss: 0.0185\n",
      "Epoch 26/360\n",
      "28/28 [==============================] - 1s 20ms/step - loss: 0.0185 - val_loss: 0.0180\n",
      "Epoch 27/360\n",
      "28/28 [==============================] - 1s 20ms/step - loss: 0.0180 - val_loss: 0.0176\n",
      "Epoch 28/360\n",
      "28/28 [==============================] - 1s 20ms/step - loss: 0.0178 - val_loss: 0.0174\n",
      "Epoch 29/360\n",
      "28/28 [==============================] - 1s 20ms/step - loss: 0.0176 - val_loss: 0.0170\n",
      "Epoch 30/360\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 0.0172 - val_loss: 0.0167\n",
      "Epoch 31/360\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 0.0172 - val_loss: 0.0165\n",
      "Epoch 32/360\n",
      "28/28 [==============================] - 1s 20ms/step - loss: 0.0165 - val_loss: 0.0163\n",
      "Epoch 33/360\n",
      "28/28 [==============================] - 1s 20ms/step - loss: 0.0167 - val_loss: 0.0160\n",
      "Epoch 34/360\n",
      "28/28 [==============================] - 1s 20ms/step - loss: 0.0162 - val_loss: 0.0158\n",
      "Epoch 35/360\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 0.0158 - val_loss: 0.0156\n",
      "Epoch 36/360\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 0.0157 - val_loss: 0.0154\n",
      "Epoch 37/360\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 0.0156 - val_loss: 0.0153\n",
      "Epoch 38/360\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 0.0155 - val_loss: 0.0152\n",
      "Epoch 39/360\n",
      "28/28 [==============================] - 1s 20ms/step - loss: 0.0153 - val_loss: 0.0151\n",
      "Epoch 40/360\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 0.0152 - val_loss: 0.0151\n",
      "Epoch 41/360\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 0.0151 - val_loss: 0.0150\n",
      "Epoch 42/360\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 0.0152 - val_loss: 0.0150\n",
      "Epoch 43/360\n",
      "28/28 [==============================] - 1s 20ms/step - loss: 0.0151 - val_loss: 0.0150\n",
      "Epoch 44/360\n",
      "28/28 [==============================] - 1s 20ms/step - loss: 0.0149 - val_loss: 0.0149\n",
      "Epoch 45/360\n",
      "28/28 [==============================] - 1s 20ms/step - loss: 0.0152 - val_loss: 0.0149\n",
      "Epoch 46/360\n",
      "28/28 [==============================] - 1s 20ms/step - loss: 0.0150 - val_loss: 0.0149\n",
      "Epoch 47/360\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 0.0151 - val_loss: 0.0148\n",
      "Epoch 48/360\n",
      "28/28 [==============================] - 1s 20ms/step - loss: 0.0150 - val_loss: 0.0148\n",
      "Epoch 49/360\n",
      "28/28 [==============================] - 1s 20ms/step - loss: 0.0149 - val_loss: 0.0148\n",
      "Epoch 50/360\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 0.0152 - val_loss: 0.0147\n",
      "Epoch 51/360\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 0.0149 - val_loss: 0.0148\n",
      "Epoch 52/360\n",
      "28/28 [==============================] - 1s 20ms/step - loss: 0.0152 - val_loss: 0.0147\n",
      "Epoch 53/360\n",
      "28/28 [==============================] - 1s 20ms/step - loss: 0.0148 - val_loss: 0.0147\n",
      "Epoch 54/360\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 0.0145 - val_loss: 0.0146\n",
      "Epoch 55/360\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 0.0148 - val_loss: 0.0146\n",
      "Epoch 56/360\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 0.0150 - val_loss: 0.0146\n",
      "Epoch 57/360\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 0.0146 - val_loss: 0.0146\n",
      "Epoch 58/360\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 0.0149 - val_loss: 0.0145\n",
      "Epoch 59/360\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 0.0146 - val_loss: 0.0145\n",
      "Epoch 60/360\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 0.0144 - val_loss: 0.0145\n",
      "Epoch 61/360\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 0.0146 - val_loss: 0.0145\n",
      "Epoch 62/360\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 0.0146 - val_loss: 0.0144\n",
      "Epoch 63/360\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 0.0147 - val_loss: 0.0145\n",
      "Epoch 64/360\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 0.0145 - val_loss: 0.0144\n",
      "Epoch 65/360\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 0.0147 - val_loss: 0.0143\n",
      "Epoch 66/360\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 0.0142 - val_loss: 0.0143\n",
      "Epoch 67/360\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 0.0142 - val_loss: 0.0143\n",
      "Epoch 68/360\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 0.0141 - val_loss: 0.0142\n",
      "Epoch 69/360\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 0.0145 - val_loss: 0.0142\n",
      "Epoch 70/360\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 0.0141 - val_loss: 0.0142\n",
      "Epoch 71/360\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 0.0145 - val_loss: 0.0142\n",
      "Epoch 72/360\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 0.0142 - val_loss: 0.0141\n",
      "Epoch 73/360\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 0.0141 - val_loss: 0.0142\n",
      "Epoch 74/360\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 0.0143 - val_loss: 0.0141\n",
      "Epoch 75/360\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 0.0140 - val_loss: 0.0141\n",
      "Epoch 76/360\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 0.0140 - val_loss: 0.0141\n",
      "Epoch 77/360\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 0.0141 - val_loss: 0.0141\n",
      "Epoch 78/360\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 0.0142 - val_loss: 0.0141\n",
      "Epoch 79/360\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 0.0139 - val_loss: 0.0141\n",
      "Epoch 80/360\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 0.0139 - val_loss: 0.0141\n",
      "Epoch 81/360\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 0.0141 - val_loss: 0.0141\n",
      "Epoch 82/360\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 0.0142 - val_loss: 0.0141\n",
      "Epoch 83/360\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 0.0139 - val_loss: 0.0140\n",
      "Epoch 84/360\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 0.0139 - val_loss: 0.0141\n",
      "Epoch 85/360\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 0.0141 - val_loss: 0.0140\n",
      "Epoch 86/360\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 0.0141 - val_loss: 0.0140\n",
      "Epoch 87/360\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 0.0141 - val_loss: 0.0140\n",
      "Epoch 88/360\n",
      "28/28 [==============================] - 1s 23ms/step - loss: 0.0139 - val_loss: 0.0140\n",
      "Epoch 89/360\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 0.0138 - val_loss: 0.0140\n",
      "Epoch 90/360\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 0.0140 - val_loss: 0.0141\n",
      "Epoch 91/360\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 0.0140 - val_loss: 0.0140\n",
      "Epoch 92/360\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 0.0142 - val_loss: 0.0140\n",
      "Epoch 93/360\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 0.0139 - val_loss: 0.0140\n",
      "Epoch 94/360\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 0.0141 - val_loss: 0.0140\n",
      "Epoch 95/360\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 0.0137 - val_loss: 0.0140\n",
      "Epoch 96/360\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 0.0138 - val_loss: 0.0140\n",
      "Epoch 97/360\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 0.0142 - val_loss: 0.0140\n",
      "Epoch 98/360\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 0.0141 - val_loss: 0.0140\n",
      "Epoch 99/360\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 0.0142 - val_loss: 0.0140\n",
      "Epoch 100/360\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 0.0139 - val_loss: 0.0139\n",
      "Epoch 101/360\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 0.0140 - val_loss: 0.0140\n",
      "Epoch 102/360\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 0.0141 - val_loss: 0.0140\n",
      "Epoch 103/360\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 0.0138 - val_loss: 0.0140\n",
      "Epoch 104/360\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 0.0139 - val_loss: 0.0139\n",
      "Epoch 105/360\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 0.0139 - val_loss: 0.0139\n",
      "Epoch 106/360\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 0.0141 - val_loss: 0.0139\n",
      "Epoch 107/360\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 0.0140 - val_loss: 0.0139\n",
      "Epoch 108/360\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 0.0140 - val_loss: 0.0140\n",
      "Epoch 109/360\n",
      "28/28 [==============================] - 1s 24ms/step - loss: 0.0136 - val_loss: 0.0139\n",
      "Epoch 110/360\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 0.0140 - val_loss: 0.0139\n",
      "Epoch 111/360\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 0.0140 - val_loss: 0.0139\n",
      "Epoch 112/360\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 0.0139 - val_loss: 0.0139\n",
      "Epoch 113/360\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 0.0139 - val_loss: 0.0139\n",
      "Epoch 114/360\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 0.0137 - val_loss: 0.0140\n",
      "Epoch 115/360\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 0.0137 - val_loss: 0.0139\n",
      "Epoch 116/360\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 0.0136 - val_loss: 0.0139\n",
      "Epoch 117/360\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 0.0137 - val_loss: 0.0139\n",
      "Epoch 118/360\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 0.0138 - val_loss: 0.0139\n",
      "Epoch 119/360\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 0.0138 - val_loss: 0.0139\n",
      "Epoch 120/360\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 0.0137 - val_loss: 0.0139\n",
      "Epoch 121/360\n",
      "28/28 [==============================] - 1s 21ms/step - loss: 0.0137 - val_loss: 0.0139\n",
      "Epoch 122/360\n",
      "28/28 [==============================] - 1s 22ms/step - loss: 0.0138 - val_loss: 0.0139\n",
      "Epoch 123/360\n",
      "14/28 [==============>...............] - ETA: 0s - loss: 0.0136"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/ht/74j_l0hj2lzg0r9j2xgjmd3r0000gn/T/ipykernel_33130/2453477422.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m#for l in [2]:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0mvae\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvaeAD\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbuild\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlatent_dim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m6\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m history = vae.fit(x_train, x_train, epochs=360, shuffle=True, batch_size=batch_size,\n\u001b[0m\u001b[1;32m     13\u001b[0m         callbacks=callbacks, validation_data=(x_val,x_val))\n\u001b[1;32m     14\u001b[0m     \u001b[0;31m#loss = np.min(history.history[\"val_loss\"])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/m1tf/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1098\u001b[0m                 _r=1):\n\u001b[1;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1101\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/m1tf/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/m1tf/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    853\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    854\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 855\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    856\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    857\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/m1tf/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2940\u001b[0m       (graph_function,\n\u001b[1;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m-> 2942\u001b[0;31m     return graph_function._call_flat(\n\u001b[0m\u001b[1;32m   2943\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m   2944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/m1tf/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1916\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1917\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1918\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1919\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[0;32m~/miniforge3/envs/m1tf/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    553\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 555\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    556\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    557\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/m1tf/lib/python3.8/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "metadata": {
    "scrolled": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#plt.plot(np.arange(2,17), val_losses[1:])\n",
    "encoder = vaeAD.get_encoder()\n",
    "decoder = vaeAD.get_decoder()\n",
    "encoder.summary()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def plot_history(history, metrics):\n",
    "\n",
    "    for metric in metrics:\n",
    "        data = history.history[metric]\n",
    "        plt.plot(data)\n",
    "    plt.legend(metrics)\n",
    "\n",
    "unsupervised_metrics = [\"loss\", \"val_loss\"]\n",
    "plot_history(history, unsupervised_metrics)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "f = 2\n",
    "plt.plot(x_train[31,:,f])\n",
    "\n",
    "pred_x = vae.predict(x_train[:64, :, :])\n",
    "\n",
    "plt.plot(pred_x[31, :, f])\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print(max(x_train[11,:,f]))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#keras.utils.plot_model(vae, show_shapes = True)"
   ],
   "outputs": [],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Aktives Lernen - Feedback f체r das Gesamtmodell\n",
    "Im un체berwachten Szenario ist der Verlust unver채ndert, im 체berwachten Szenario wird auf den Verlus der BinaryCrossEntropyLoss des Klassifikator aufaddiert"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "activeAD = ED_Feedback(vaeAD)\n",
    "keras.utils.plot_model(activeAD.get_supervised(), show_shapes=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#keras.utils.plot_model(vae_al, show_shapes = True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def attention_visualizer():\n",
    "    \n",
    "    (orig, _), (_, _) = aursad()\n",
    "    \n",
    "    attention_vis = Model(inputs=encoder_input, outputs=attention_scores)\n",
    "    \n",
    "    scores = attention_vis.predict(orig[:batch_size,:,:])[0,:,:]\n",
    "    scores = np.log(scores)\n",
    "    print(\"scores: \",scores.shape)\n",
    "    #plt.imshow(scores, cmap='hot', interpolation='nearest')\n",
    "    #plt.show()\n",
    "    \n",
    "    n = scores.shape[0]\n",
    "    n = n//10\n",
    "    \n",
    "    print(np.sum(scores, axis=1))\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1,2,gridspec_kw={'width_ratios': [3, 1]}, figsize=(10,6))\n",
    "    im = ax1.imshow(scores)\n",
    "    \n",
    "    ax1.set_xticks(np.arange(10)*n)\n",
    "    ax1.set_yticks(np.arange(10)*n)\n",
    "\n",
    "    # Loop over data dimensions and create text annotations.\n",
    "    for i in range(10):\n",
    "        for j in range(10):\n",
    "            text = ax1.text(j*n, i*n, np.round(scores[i*n, j*n],2), ha=\"center\", va=\"center\", color=\"w\")\n",
    "\n",
    "    ax1.set_title(\"Attention heat map\")\n",
    "    \n",
    "    ax2.plot(orig[0,:,0],-np.arange(0,len(orig[0,:,0])))\n",
    "    ax2.set_ylim(-len(orig[0,:,0]),0)\n",
    "    \n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "   \n",
    "#attention_visualizer()"
   ],
   "outputs": [],
   "metadata": {
    "scrolled": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def color_from_label(labels):\n",
    "    l2c = {0:'tab:cyan',\n",
    "           1:'tab:blue',\n",
    "           2:'tab:green',\n",
    "           3:'tab:olive',\n",
    "           4:'tab:orange',\n",
    "           5:'tab:red',\n",
    "           6:'tab:brown',\n",
    "           7:'tab:purple',\n",
    "           8:'tab:pink',\n",
    "           9:'tab:gray',\n",
    "          }\n",
    "    return [l2c[label] for label in labels]\n",
    "\n",
    "\n",
    "def latent_space_pca(dim=2, dataset='train'):\n",
    "\n",
    "    x, y = supervised(dataset=dataset)\n",
    "    pca = PCA(n_components=dim)\n",
    "\n",
    "    z = encoder.predict(x)\n",
    "    z_pca = pca.fit_transform(z)\n",
    "\n",
    "    if dim==3:\n",
    "        from mpl_toolkits import mplot3d\n",
    "        fig = plt.figure(figsize=(8,8))\n",
    "        ax = plt.axes(projection='3d')\n",
    "        ax.scatter3D(z_pca[:,0], z_pca[:,1], z_pca[:,2], c=color_from_label(y), alpha=0.5)\n",
    "\n",
    "    plt.figure(figsize=(15,15))\n",
    "    plt.scatter(z_pca[:,0], z_pca[:,1], c=color_from_label(y))\n",
    "\n",
    "def visualize_latent_space(show_size=False, dataset='test'):\n",
    "\n",
    "    x, y = supervised(dataset=dataset)\n",
    "\n",
    "    z = encoder.predict(x)\n",
    "    z_1 = z[:,0]\n",
    "    z_2 = z[:,1]\n",
    "\n",
    "    plt.figure(figsize=(15,15))\n",
    "    plt.scatter(z_1, z_2, c=color_from_label(y))\n",
    "\n",
    "#visualize_latent_space()\n",
    "latent_space_pca(dim=2)"
   ],
   "outputs": [],
   "metadata": {
    "scrolled": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def mahalanobis_anomaly_score_series(x_true, x_pred):\n",
    "    \n",
    "    a = np.zeros((x_true.shape[0], x_true.shape[1]))\n",
    "    for t in range(x_true.shape[1]):\n",
    "        \n",
    "        xt_true = x_true[:,t,:]\n",
    "        xt_pred = x_pred[:,t,:]\n",
    "        \n",
    "        error = np.abs(xt_true-xt_pred)\n",
    "    \n",
    "        mu = np.mean(error, axis=0)\n",
    "        #print(mu.shape)\n",
    "        \n",
    "        cov = np.cov(error, rowvar=False)\n",
    "        inv_cov = np.linalg.inv(cov)\n",
    "        #print(cov.shape)\n",
    "        \n",
    "        for s in range(x_true.shape[0]):\n",
    "            e = error[s,:]\n",
    "\n",
    "            a[s,t] = np.matmul(np.matmul((e-mu).T, inv_cov),(e-mu))\n",
    "        \n",
    "    return a\n",
    "\n",
    "def show_anomaly_score(sample=0, label=0):\n",
    "    \n",
    "    orig, y = supervised()    \n",
    "    x_pred = vae.predict(orig)\n",
    "\n",
    "    a = mahalanobis_anomaly_score_series(orig, x_pred)\n",
    "    \n",
    "    s = 0\n",
    "    while y[s] != label:\n",
    "        s+=1   \n",
    "            \n",
    "    num_plots = orig.shape[2]+1\n",
    "    fig, ax = plt.subplots(num_plots,1,figsize=(15,num_plots*3))\n",
    "        \n",
    "    for i, axi in enumerate(ax):\n",
    "            \n",
    "        if axi != ax[-1]:\n",
    "            axi.plot(orig[s,:,i])\n",
    "            axi.plot(x_pred[s,:,i])\n",
    "        else:\n",
    "            break\n",
    "        \n",
    "    ax[-1].plot(a[s,:], c=\"r\")\n",
    "    ax[-1].hlines(np.mean(a[s,:]), 0,400)\n",
    "    ax[-1].legend([\"mahalanobis\", str(np.mean(a[s,:]))])\n",
    "    plt.show()\n",
    "        \n",
    "show_anomaly_score(label=0)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#def load_vae(path):\n",
    "#    mapping = {\n",
    "#        \"KLDivergenceLayer\": KLDivergenceLayer,\n",
    "#        \"SampleLayer\": SampleLayer,\n",
    "#        \"ReconstructionLossLayer\": ReconstructionLossLayer,\n",
    "#    }\n",
    "#    vae = keras.models.load_model(path, custom_objects = mapping)\n",
    "#    return vae"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def mse(x_true, x_pred):\n",
    "        \n",
    "        loss = np.mean((x_true-x_pred)**2, axis=1)\n",
    "        loss = np.mean(loss, axis=-1)\n",
    "        return loss\n",
    "    \n",
    "def mae(x_true, x_pred):\n",
    "    loss = np.mean(np.abs(x_true-x_pred), axis=1)\n",
    "    loss = np.mean(loss, axis=-1)\n",
    "    return loss\n",
    "\n",
    "def mahalanobis_anomaly_score(x_true, x_pred):\n",
    "    \"\"\"Reduces the mahalanobis anomaly score series to a scalar\"\"\"\n",
    "    series = mahalanobis_anomaly_score_series(x_true, x_pred)\n",
    "    return np.mean(series, axis=1)\n",
    "\n",
    "def anomaly_detection():\n",
    "    \n",
    "    x_test, y_test = supervised(dataset=\"test\")\n",
    "    x_pred = vae.predict(x_test)\n",
    "\n",
    "    anomaly_scores = mahalanobis_anomaly_score(x_test, x_pred)\n",
    "\n",
    "    all_scores = list()\n",
    "    labels = [i for i in range(4)]\n",
    "\n",
    "    for label in labels:\n",
    "        scores = anomaly_scores[y_test== label]\n",
    "        if len(scores) == 0:\n",
    "            all_scores.append([0])\n",
    "        else:\n",
    "            all_scores.append(scores)\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(8,12))\n",
    "    ax.boxplot(all_scores,\n",
    "        vert=True,  # vertical box alignment\n",
    "        patch_artist=True,  # fill with color\n",
    "        labels=labels)\n",
    "    ax.set_ylabel(\"anomaly score\")\n",
    "    \n",
    "    x_train, y = unsupervised()\n",
    "    print(x_train.shape)\n",
    "    print(np.sum(y!=0))\n",
    "    \n",
    "    x_pred = vae.predict(x_train)\n",
    "    anomaly_scores = mahalanobis_anomaly_score(x_train, x_pred)\n",
    "\n",
    "    critical_anomaly_score = np.quantile(anomaly_scores, 0.98)\n",
    "    print(critical_anomaly_score)\n",
    "    \n",
    "    ax.axhline(critical_anomaly_score, color=\"r\")\n",
    "\n",
    "#my_vae = load_vae(model_str)\n",
    "anomaly_detection()"
   ],
   "outputs": [],
   "metadata": {
    "scrolled": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def quantile_threshold(p, loss_func=mae, return_loss=False):\n",
    "    \n",
    "    x, _ = supervised(dataset='train')\n",
    "    x_hat = vae.predict(x)\n",
    "\n",
    "    loss = loss_func(x, x_hat)\n",
    "    threshold = np.quantile(loss, p)\n",
    "\n",
    "    if return_loss:\n",
    "        return threshold, loss\n",
    "    else:\n",
    "        return threshold\n",
    "\n",
    "def confusion_matrix(actual, predicted):\n",
    "    \"\"\"Computes the elements of a confusion matrix: tp, fp, fn, tn\n",
    "    \"\"\"\n",
    "\n",
    "    # positiv = anomalous, negative = normal\n",
    "    tp = np.sum(np.logical_and(actual == 1, predicted == 1))\n",
    "    tn = np.sum(np.logical_and(actual == 0, predicted == 0))\n",
    "    fp = np.sum(np.logical_and(actual == 0, predicted == 1))\n",
    "    fn = np.sum(np.logical_and(actual == 1, predicted == 0))\n",
    "    \n",
    "    return tp, fp, fn, tn\n",
    "\n",
    "def accuracy(tp, fp, fn, tn):  \n",
    "    return (tp+tn) / (tp+fp+fn+tn)\n",
    "\n",
    "def precision(tp, fp, fn, tn):\n",
    "    return tp/(tp+fp)\n",
    "\n",
    "def recall(tp, fp, fn, tn):\n",
    "    return tp/(tp+fn)\n",
    "\n",
    "def f_score(tp, fp, fn, tn):\n",
    "    p = precision(tp, fp, fn, tn)\n",
    "    r = recall(tp, fp, fn, tn)\n",
    "    return 2*p*r/(p+r)\n",
    "\n",
    "def mcc_score(tp, fp, fn, tn):\n",
    "    return (tp*tn-fp*fn)/np.sqrt((tp+fp)*(tp+fn)*(tn+fp)*(tn+fn))\n",
    "\n",
    "def baseline_ad(model = \"unsupervised\", loss_func=mae):\n",
    "    \n",
    "    ##x_train, _ = unsupervised(percentage=0.125)\n",
    "    #x_train, _ = supervised(dataset=\"train\")\n",
    "    #    \n",
    "    #x_pred = vae.predict(x_train)\n",
    "    #training_loss = loss_func(x_train, x_pred)    \n",
    "    #    \n",
    "    #critical_anomaly_score = np.quantile(training_loss, 0.865)\n",
    "\n",
    "    critical_anomaly_score = quantile_threshold(0.865, loss_func)\n",
    "    \n",
    "    print(f'Critical Anomaly Score = {critical_anomaly_score}')\n",
    "    \n",
    "    x_test, y_test = supervised(dataset=\"test\")\n",
    "    \n",
    "    for i in range(5):\n",
    "        count = np.sum(y_test==i)\n",
    "        print(f'Anzahl {i}: {count}')\n",
    "    \n",
    "    x_test_pred = vae.predict(x_test)\n",
    "    test_loss = loss_func(x_test, x_test_pred)    \n",
    "    \n",
    "    predicted = test_loss > critical_anomaly_score\n",
    "    actual = y_test != 0\n",
    "    \n",
    "    tp, fp, fn, tn = confusion_matrix(actual, predicted)\n",
    "    n = len(y_test)\n",
    "    \n",
    "    print(f'TP = {tp}, TN = {tn}, FP = {fp}, FN = {fn}, N = {n}')\n",
    "    \n",
    "    acc = accuracy(tp, fp, fn, tn)\n",
    "    prec = precision(tp, fp, fn, tn)\n",
    "    rec = recall(tp, fp, fn, tn)\n",
    "    f1 = f_score(tp, fp, fn, tn)\n",
    "    mcc = mcc_score(tp, fp, fn, tn)\n",
    "    print(f'Accuracy = {acc}, F1 = {f1}, precision = {prec}, recall = {rec}, mcc = {mcc}')\n",
    "    \n",
    "    \n",
    "baseline_ad(loss_func=mahalanobis_anomaly_score)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Query Strategy"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def naive_query(budget, return_idx=False):\n",
    "    # query the entire, non-restricted \n",
    "    x, y = unsupervised(percentage=0.99)\n",
    "    y = y != 0\n",
    "\n",
    "    x = x[:budget,:,:]  \n",
    "    y = y[:budget]   \n",
    "    \n",
    "    if return_idx:\n",
    "        return (x,y), np.arange(budget)\n",
    "    else:\n",
    "        return x, y\n",
    "    \n",
    "def edgecase_query(budget, return_idx=False):\n",
    "    # query the entire, non-restricted \n",
    "    x, y = unsupervised(percentage=0.99)\n",
    "    y = y != 0\n",
    "\n",
    "    threshold, score = quantile_threshold(0.865,\n",
    "                            loss_func=mahalanobis_anomaly_score, return_loss=True)\n",
    "    #print(f'Threshold anomaly score: {threshold}')\n",
    "        \n",
    "    edge_cases = np.abs(score - threshold)    \n",
    "    idx = np.argsort(edge_cases)\n",
    "    idx = idx[:budget]\n",
    "        \n",
    "    if return_idx:\n",
    "        return (x[idx,::], y[idx]), idx\n",
    "    else:\n",
    "        return x[idx,::], y[idx]\n",
    "    \n",
    "def most_anomalous_query(budget, return_idx=False):\n",
    "    # query the entire, non-restricted \n",
    "    x, y = unsupervised(percentage=0.99)\n",
    "    y = y != 0\n",
    "    \n",
    "    x_hat = vae.predict(x)\n",
    "    score = mahalanobis_anomaly_score(x, x_hat)\n",
    "\n",
    "    idx = np.argsort(score)\n",
    "    idx = idx[-budget:]\n",
    "        \n",
    "    if return_idx:\n",
    "        return (x[idx,::], y[idx]), idx\n",
    "    else:\n",
    "        return x[idx,::], y[idx]\n",
    "\n",
    "def normal_query(budget, return_idx=False):\n",
    "    # query the entire, non-restricted \n",
    "    x, y = unsupervised(percentage=0.99)\n",
    "    y = y != 0\n",
    "    \n",
    "    x_hat = vae.predict(x)\n",
    "    score = mahalanobis_anomaly_score(x, x_hat)\n",
    "\n",
    "    idx = np.argsort(score)\n",
    "    idx = idx[:budget]\n",
    "        \n",
    "    if return_idx:\n",
    "        return (x[idx,::], y[idx]), idx\n",
    "    else:\n",
    "        return x[idx,::], y[idx]\n",
    "\n",
    "\n",
    "def clustering_query(budget, plot=False, return_idx=False):\n",
    "    # query the entire, non-restricted \n",
    "    x, y = unsupervised(percentage=0.99)\n",
    "    y = y != 0\n",
    "      \n",
    "    def dbscan_clustering(z, sample_weight=None):\n",
    "        y = DBSCAN(min_samples=9, eps=0.1).fit_predict(z, sample_weight=sample_weight)\n",
    "        return y\n",
    "    \n",
    "    def kmeans_clustering(z, sample_weight=None):\n",
    "        kmeans = KMeans(n_clusters=10, random_state=0).fit(z, sample_weight=sample_weight)\n",
    "        return kmeans.predict(z)\n",
    "    \n",
    "\n",
    "    threshold, score = quantile_threshold(0.865,\n",
    "                            loss_func=mahalanobis_anomaly_score, return_loss=True)\n",
    "\n",
    "    sample_weight = score > threshold\n",
    "    sample_weight = sample_weight * 3 + 1\n",
    "\n",
    "    pca = PCA(n_components=2)\n",
    "    z = encoder.predict(x)\n",
    "    z_pca = pca.fit_transform(z)\n",
    "\n",
    "    y_cluster = dbscan_clustering(z_pca, sample_weight=sample_weight)\n",
    "    \n",
    "    import pandas as pd\n",
    "    y_series = pd.Series(y_cluster)\n",
    "\n",
    "    #print(y_series)\n",
    "    grouped = y_series.groupby(y_series).count().reset_index()\n",
    "    #print(grouped.columns)\n",
    "    grouped=grouped.sort_values(0)\n",
    "    #print(grouped)\n",
    "\n",
    "    counts = grouped[0]\n",
    "    #print(counts)\n",
    "\n",
    "    idx = list()\n",
    "    for j, (i, _) in enumerate(counts.iteritems()):\n",
    "        \n",
    "        remaining = budget - len(idx)\n",
    "        samples_per_label = int(np.ceil(remaining/(len(counts)-j)))\n",
    "        \n",
    "        pos = np.arange(len(y_series))\n",
    "        new_idx = pos[y_series==(i-1)][:samples_per_label]\n",
    "        idx.extend(new_idx)\n",
    "        #print(i,_,len(new_idx))\n",
    "\n",
    "    if plot:\n",
    "        print(np.unique(y_cluster))\n",
    "        fig = plt.figure(figsize=(12,12))\n",
    "        plt.scatter(z_pca[:,0], z_pca[:,1],c=y_cluster, s=100, cmap=cm.tab20)\n",
    "        z_neg = z_pca[y_cluster<0,:]\n",
    "        plt.scatter(z_neg[:,0], z_neg[:,1],c='k', s=100)\n",
    "        plt.scatter(z_pca[idx,0], z_pca[idx,1], edgecolors='r', s=160, marker='o', facecolors='none')\n",
    "\n",
    "    if return_idx:\n",
    "        return (x[idx,::], y[idx]), idx\n",
    "    else:\n",
    "        return x[idx,::], y[idx]\n",
    "    \n",
    "#_,_ = clustering_query(128, plot=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## E2: Vergleich der Abfragestrategien\n",
    "\n",
    "F체r die Ergebnisse des CNN-LSTM Modell werden die verschiedenen Abfragestrategien verglichen\n",
    "\n",
    "Darstellungen:\n",
    "* a - Quantitativ: Wie viele Beispiele der einzelnen Klassen werden ausgew채hlt?\n",
    "* b - Qualitativ: Welche Beispiele (Lage im latenten Raum) der einzelnen Klassen werden ausgew채hlt?\n",
    "* c - Wirkung: Welche Leistungsf채higkeit des 체berwachten Modells ist damit erreichbar?"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def e2a():\n",
    "    budget = 128\n",
    "\n",
    "    queries = [naive_query, edgecase_query, most_anomalous_query, normal_query, clustering_query]\n",
    "    for query in queries:\n",
    "        qx, qy = query(budget)\n",
    "\n",
    "        num_normal = np.sum(qy == 0)\n",
    "        num_anomalies = np.sum(qy != 0)\n",
    "        print(f\"Query: {query.__name__}, Normal: {num_normal}, anomalies {num_anomalies}\")\n",
    "\n",
    "e2a()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from matplotlib.gridspec import GridSpec\n",
    "from matplotlib.colors import Normalize\n",
    "import pandas as pd\n",
    "\n",
    "def e2b():\n",
    "    # query the entire, non-restricted \n",
    "    x, y = unsupervised(percentage=0.99)\n",
    "    y = y != 0\n",
    "    \n",
    "    pca = PCA(n_components=2)\n",
    "    z = encoder.predict(x)\n",
    "    z_pca = pca.fit_transform(z)\n",
    "\n",
    "    x_hat = vae.predict(x)\n",
    "    threshold, score = quantile_threshold(0.865,\n",
    "                            loss_func=mahalanobis_anomaly_score, return_loss=True)\n",
    "    \n",
    "    # make the scores logarithmic for a enhanced depiction\n",
    "    threshold, score = np.log(threshold), np.log(score)\n",
    "\n",
    "    (_,_), idx = most_anomalous_query(budget=64, return_idx=True)\n",
    "\n",
    "    fig= plt.figure(figsize=(12,8))\n",
    "    grid_spec = fig.add_gridspec(ncols=2, nrows=2, height_ratios=[8,1], width_ratios=[9.8, 0.2])\n",
    "\n",
    "    ax1 = fig.add_subplot(grid_spec[0,0])\n",
    "    ax2 = fig.add_subplot(grid_spec[1,:])\n",
    "    axcb = fig.add_subplot(grid_spec[0,1])\n",
    "\n",
    "    ax1.scatter(z_pca[:,0], z_pca[:,1], c=score, cmap=cm.RdYlGn_r)\n",
    "    ax1.scatter(z_pca[idx,0], z_pca[idx,1], edgecolors='r', marker='o', facecolors='none', s=160)\n",
    "\n",
    "    clb = fig.colorbar(cm.ScalarMappable(cmap=cm.RdYlGn_r, norm=Normalize(vmin=np.min(score), vmax=np.max(score))), cax=axcb)\n",
    "    clb.set_label('log(Anomaliescore)', fontsize=14)\n",
    "    ax2.scatter(score, np.ones(len(score)), s=20, c='k', label='Beispiel')\n",
    "    ax2.set_yticks([])\n",
    "    ax2.scatter(score[idx], np.ones(len(idx)), edgecolors='r', marker='o', facecolors='none', s=100, label='abgefragt')\n",
    "    ax2.vlines(threshold, 0.95, 1.05, linestyle=\"--\", color=\"tab:orange\", label=\"Schwellenwert\")\n",
    "    ax2.set_xlabel(\"log(Anomaliescore)\", fontsize=14)\n",
    "    ax2.legend(ncol=3, loc='upper center', bbox_to_anchor=(0.5,-0.6), fontsize=14)\n",
    "    plt.tight_layout()\n",
    "\n",
    "    #df1 = pd.DataFrame({'score':np.log(score), 'queried':[\"Beispiele\"]*len(score)})\n",
    "    #df2 = pd.DataFrame({'score':np.log(score[idx]), 'queried':[\"ausgew채hlt\"]*len(score[idx])})\n",
    "    #df = pd.concat([df1, df2])\n",
    "    #df['all'] = ''\n",
    "    #ax = sns.violinplot(y='all', x='score', data=df, hue='queried', \n",
    "    #        split=True, palette=\"Set2\", scale='count', inner=None)\n",
    "\n",
    "# e2b()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def active_learning(x_query, y_query, strategy='none'):\n",
    "    \n",
    "    budget = len(y_query)\n",
    "    model_file = os.path.join(output_path, f'b{budget}-s{strategy}-{model_str}')\n",
    "    y_query = keras.utils.to_categorical(y_query, num_classes=2)\n",
    "\n",
    "    callbacks = [\n",
    "        #keras.callbacks.ModelCheckpoint(\n",
    "        #    monitor = 'loss',\n",
    "        #    filepath = model_file, \n",
    "        #    save_weights_only = False,\n",
    "        #    save_best_only = True),\n",
    "        keras.callbacks.EarlyStopping(monitor='loss', patience=8)\n",
    "        ]\n",
    "\n",
    "    # warm-up\n",
    "    #vae.fit(x_query, x_query, epochs=1, shuffle=True, batch_size=batch_size)\n",
    "\n",
    "    history = activeAD.get_supervised().fit(x_query, y_query, shuffle = True, \n",
    "                epochs=180, batch_size=64, callbacks=callbacks)\n",
    "    #activeAD.get_supervised().load_weights(model_file)\n",
    "    \n",
    "    plot_history(history, ['loss'])\n",
    "    with open(os.path.join(output_path, \"active-history-1.json\"), 'w') as file:\n",
    "            json.dump(history.history, file)\n",
    "\n",
    "    x_test, y_test = supervised(dataset = \"test\")\n",
    "    y_test = y_test != 0\n",
    "    y_test = keras.utils.to_categorical(y_test, num_classes=2)\n",
    "    \n",
    "    predicted = activeAD.get_supervised().predict(x_test)\n",
    "    predicted = np.argmax(predicted, axis=-1)\n",
    "    actual = np.argmax(y_test, axis=-1)\n",
    "\n",
    "    tp, fp, fn, tn = confusion_matrix(actual, predicted)\n",
    "\n",
    "    tp, fp, fn, tn = confusion_matrix(actual, predicted)\n",
    "    acc = accuracy(tp, fp, fn, tn)\n",
    "    prec = precision(tp, fp, fn, tn)\n",
    "    rec = recall(tp, fp, fn, tn)\n",
    "    f1 = f_score(tp, fp, fn, tn)\n",
    "    mcc = mcc_score(tp, fp, fn, tn)\n",
    "        \n",
    "    print(f'TP = {tp}, TN = {tn}, FP = {fp}, FN = {fn}')\n",
    "    print(f'Accuracy = {acc}, F1 = {f1}, precision = {prec}, recall = {rec}, mcc = {mcc}')\n",
    "\n",
    "def e2c():\n",
    "    qx1,qy1 = normal_query(budget=128)\n",
    "    qx2,qy2 = most_anomalous_query(budget=256)\n",
    "\n",
    "    qx = np.concatenate((qx1, qx2), axis=0)\n",
    "    qy = np.concatenate((qy1, qy2), axis=0)\n",
    "    \n",
    "    active_learning(qx, qy, strategy='M')\n",
    "\n",
    "e2c()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## E3: Feedback zur F-Score Optimierune\n",
    "\n",
    "Vorteile F-Score:\n",
    "* einfach zu berechnen\n",
    "* gebr채uchig f체r ML\n",
    "* Wertebereich [0,1]\n",
    "* Goodfellow, p. 423f\n",
    "* Hand, David\n",
    "\n",
    "Vorteile MCC:\n",
    "* unabh채ngig, welche Klasse als positiv / negativ deklariert wird\n",
    "* basiert direkt auf der Konfusionsmatirx, unabh채ngig von Imbalance"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def e3():\n",
    "    xq, yq = naive_query(budget=256) \n",
    "    xq_hat = vae.predict(xq)\n",
    "    score = mahalanobis_anomaly_score(xq, xq_hat)\n",
    "\n",
    "    minScore = np.min(score)\n",
    "    maxScore = np.max(score)\n",
    "    scores = np.arange(minScore, maxScore, 0.01)\n",
    "\n",
    "    opt = -1\n",
    "    threshold = None\n",
    "    for s in scores:\n",
    "        y_pred = score > s\n",
    "        tp, fp, fn, tn = confusion_matrix(yq, y_pred)\n",
    "        new = f_score(tp, fp, fn, tn)\n",
    "        if new > opt:\n",
    "            opt = new\n",
    "            threshold=s\n",
    "    \n",
    "    # report\n",
    "    print(f'opt. score (query dataset): {opt} @ opt. threshold: {threshold}')\n",
    "\n",
    "    # final evaluation on test data\n",
    "    x_test, y_test = supervised(dataset='test')\n",
    "    actual = y_test != 0\n",
    "\n",
    "    x_test_hat = vae.predict(x_test)\n",
    "    test_score = mahalanobis_anomaly_score(x_test, x_test_hat)\n",
    "\n",
    "    predicted = test_score > threshold\n",
    "\n",
    "    tp, fp, fn, tn = confusion_matrix(actual, predicted)\n",
    "    acc = accuracy(tp, fp, fn, tn)\n",
    "    prec = precision(tp, fp, fn, tn)\n",
    "    rec = recall(tp, fp, fn, tn)\n",
    "    f1 = f_score(tp, fp, fn, tn)\n",
    "    mcc = mcc_score(tp, fp, fn, tn)\n",
    "        \n",
    "    print(f'TP = {tp}, TN = {tn}, FP = {fp}, FN = {fn}')\n",
    "    print(f'Accuracy = {acc}, F1 = {f1}, precision = {prec}, recall = {rec}, mcc = {mcc}')\n",
    "\n",
    "#e3()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## E4: Vergleich der Architekturen\n",
    "F체r die Abfragestrategie `most_anomalous_query` erg채nzt um die `normal_query` werden die verschiedenen Architekturen bei B=256 verglichen."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def e4():\n",
    "    qx1,qy1 = normal_query(budget=128)\n",
    "    qx2,qy2 = most_anomalous_query(budget=256)\n",
    "\n",
    "    qx = np.concatenate((qx1, qx2), axis=0)\n",
    "    qy = np.concatenate((qy1, qy2), axis=0)\n",
    "    \n",
    "    active_learning(qx, qy, strategy='MN')\n",
    "#e4()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Restrukturierung des latenten Raums"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def z_shift():\n",
    "    x, y = supervised(dataset='train')\n",
    "    z_before = encoder.predict(x)\n",
    "\n",
    "    qx1,qy1 = normal_query(budget=128)\n",
    "    qx2,qy2 = most_anomalous_query(budget=256)\n",
    "\n",
    "    qx = np.concatenate((qx1, qx2), axis=0)\n",
    "    qy = np.concatenate((qy1, qy2), axis=0)\n",
    "    \n",
    "    active_learning(qx, qy, strategy='MN')\n",
    "\n",
    "    z_after = encoder.predict(x)\n",
    "\n",
    "    return z_before, z_after, y\n",
    "\n",
    "z_b, z_a, groundtruth = z_shift()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def plot_z_shift(z_b, z_a, y):\n",
    "    fig, axs = plt.subplots(ncols=2, figsize=(12,6))\n",
    "\n",
    "    def single_view(ax, z, y):\n",
    "\n",
    "        pca = PCA(n_components=2)\n",
    "        z_pca = pca.fit_transform(z)\n",
    "\n",
    "        ax.scatter(z_pca[:,0], z_pca[:,1], c=y)\n",
    "\n",
    "    single_view(axs[0], z_b, y)\n",
    "    single_view(axs[1], z_a, y)\n",
    "\n",
    "plot_z_shift(z_b, z_a, groundtruth)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "assert False"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "qx1,qy1 = normal_query(budget=128)\n",
    "qx2,qy2 = most_anomalous_query(budget=256)\n",
    "\n",
    "qx = np.concatenate((qx1, qx2), axis=0)\n",
    "qy = np.concatenate((qy1, qy2), axis=0)\n",
    "\n",
    "print(np.sum(qy1!=0), np.sum(qy2!=0))\n",
    "print(np.shape(qx))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#vae.fit(qx, qx, epochs=1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def active_learning(x_query, y_query):\n",
    "    \n",
    "    y_query = keras.utils.to_categorical(y_query, num_classes=2)\n",
    "    \n",
    "    \n",
    "    #callbacks = [keras.callbacks.ModelCheckpoint(\n",
    "    #    monitor = 'loss',\n",
    "    #    filepath = 'vae_al_ef0-{epoch:02d}.hdf5', \n",
    "    #    save_weights_only = False,\n",
    "    #    save_best_only = False)]\n",
    "\n",
    "    history = activeAD.get_supervised().fit(x_query, y_query, shuffle = True, epochs=40, batch_size=32)\n",
    "    \n",
    "    x_test, y_test = supervised(dataset = \"test\")\n",
    "    y_test = y_test != 0\n",
    "    y_test = keras.utils.to_categorical(y_test, num_classes=2)\n",
    "    \n",
    "    predicted = activeAD.get_supervised().predict(x_test)\n",
    "    #print(predicted)\n",
    "\n",
    "    predicted = np.argmax(predicted, axis=-1)\n",
    "    actual = np.argmax(y_test, axis=-1)\n",
    "\n",
    "    #print(predicted.shape, predicted)\n",
    "    #print(actual.shape, actual)\n",
    "    \n",
    "    # positiv = anomalous, negative = normal\n",
    "    n = len(y_test)\n",
    "    tp = np.sum(np.logical_and(actual == 1, predicted == 1))\n",
    "    tn = np.sum(np.logical_and(actual == 0, predicted == 0))\n",
    "    fp = np.sum(np.logical_and(actual == 0, predicted == 1))\n",
    "    fn = np.sum(np.logical_and(actual == 1, predicted == 0))\n",
    "    \n",
    "    print(f'TP = {tp}, TN = {tn}, FP = {fp}, FN = {fn}, N = {n}')\n",
    "      \n",
    "    accuracy = (tp+tn) / n\n",
    "    precision = tp / (tp+fp)\n",
    "    recall = tp / (tp+fn)\n",
    "    f1 = 2 * precision * recall / (precision + recall)\n",
    "    print(f'Accuracy = {accuracy}, F1 = {f1}, precision = {precision}, recall = {recall}')\n",
    "\n",
    "#vae.load_weights(model_str)  \n",
    "active_learning(qx, qy)"
   ],
   "outputs": [],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def active_learning_modelfeedback(normal = 0):\n",
    "    budget = batch_size \n",
    "    \n",
    "    x_query, y_query = most_anomalous_query(budget)\n",
    "    print(np.sum(y_query))\n",
    "    \n",
    "    callbacks = [keras.callbacks.ModelCheckpoint(\n",
    "        monitor = 'loss',\n",
    "        filepath = 'vae_al_ef0-{epoch:02d}.hdf5', \n",
    "        save_weights_only = False,\n",
    "        save_best_only = False)]\n",
    "\n",
    "    history = vae_al.fit(x_query, y_query, shuffle = True, epochs=60, batch_size=32)\n",
    "    \n",
    "    x_test, y_test = supervised(dataset = \"test\")\n",
    "    \n",
    "    predicted = vae_al.predict(x_test).reshape(-1) > 0.5\n",
    "    actual = (y_test != normal)\n",
    "    \n",
    "    # positiv = anomalous, negative = normal\n",
    "    tp = np.sum(np.logical_and(actual == 1, predicted == 1))\n",
    "    tn = np.sum(np.logical_and(actual == 0, predicted == 0))\n",
    "    fp = np.sum(np.logical_and(actual == 0, predicted == 1))\n",
    "    fn = np.sum(np.logical_and(actual == 1, predicted == 0))\n",
    "    n = len(actual)\n",
    "    \n",
    "    print(f'TP = {tp}, TN = {tn}, FP = {fp}, FN = {fn}, N = {n}')\n",
    "    \n",
    "    accuracy = (tp+tn) / n\n",
    "    precision = tp / (tp+fp)\n",
    "    recall = tp / (tp+fn)\n",
    "    f1 = 2 * precision * recall / (precision + recall)\n",
    "    print(f'Accuracy = {accuracy}, F1 = {f1}, precision = {precision}, recall = {recall}')\n",
    "    \n",
    "#active_learning_modelfeedback()"
   ],
   "outputs": [],
   "metadata": {
    "scrolled": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def active_learning_decoupled(normal = 0):\n",
    "    \"\"\"Enhance the prediction accuracy by retraining the model with the top-k most intersting samples in a\n",
    "    supervised fashion.\n",
    "        1) Find the top-k samples and get their labels\n",
    "        2) Retrain the model with these samples in a supervised fashion -> prevent overfitting\n",
    "        3) Evaluate accuracy, F1-score, ...\"\"\"\n",
    "    \n",
    "    budget = batch_size  \n",
    "    \n",
    "    x_query, y_query = most_anomalous_query(budget)\n",
    "    print(np.sum(y_query))\n",
    "    \n",
    "    callbacks = [keras.callbacks.ModelCheckpoint(\n",
    "        monitor = 'loss',\n",
    "        filepath = 'vae_al0-{epoch:02d}.hdf5', \n",
    "        save_weights_only = False,\n",
    "        save_best_only = False)]\n",
    "    \n",
    "    z_query = encoder.predict(x_query)\n",
    "\n",
    "    history = vae_al.fit(z_query, y_query, shuffle = True, epochs=100, batch_size=32)\n",
    "        #training_data,\n",
    "        #shuffle=True,      \n",
    "     #   callbacks = callbacks,\n",
    "      #  validation_data = (x_valid, y_valid))\n",
    "    \n",
    "    x_test, y_test = supervised(dataset = \"test\")\n",
    "    z_test = encoder.predict(x_test)\n",
    "    \n",
    "    predicted = vae_al.predict(z_test).reshape((-1)) > 0.5\n",
    "    actual = y_test != normal\n",
    "    \n",
    "    # positiv = anomalous, negative = normal\n",
    "    tp = np.sum(np.logical_and(actual == 1, predicted == 1))\n",
    "    tn = np.sum(np.logical_and(actual == 0, predicted == 0))\n",
    "    fp = np.sum(np.logical_and(actual == 0, predicted == 1))\n",
    "    fn = np.sum(np.logical_and(actual == 1, predicted == 0))\n",
    "    n = len(y_test)\n",
    "    \n",
    "    print(f'TP = {tp}, TN = {tn}, FP = {fp}, FN = {fn}, N = {n}')\n",
    "    \n",
    "    accuracy = (tp+tn) / n\n",
    "    precision = tp / (tp+fp)\n",
    "    recall = tp / (tp+fn)\n",
    "    f1 = 2 * precision * recall / (precision + recall)\n",
    "    print(f'Accuracy = {accuracy}, F1 = {f1}, precision = {precision}, recall = {recall}')\n",
    "    \n",
    "#active_learning_decoupled()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "visualize_latent_space(None, show_size = True)\n",
    "anomaly_detection(vae)"
   ],
   "outputs": [],
   "metadata": {
    "scrolled": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "a, b = supervised(dataset=\"train\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "len(b)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "np.sum(b!=0)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "96ba8bdb9944f4077aea7f2f55699da4fa58fb876a32e9ef6c1a749faec584e6"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit ('m1tf': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}