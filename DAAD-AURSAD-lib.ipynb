{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import json\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "from matplotlib.colors import Normalize\n",
    "\n",
    "from sklearn.cluster import KMeans, DBSCAN\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "from machine_learning.encoders import WaveNet_Encoder, CNN_Encoder, CNN_LSTM_Encoder, CNN_LSTM_SA_Encoder, LSTM_Encoder\n",
    "from machine_learning.decoders import CNN_Decoder, WaveNet_Decoder, CNN_LSTM_Decoder, CNN_LSTM_SA_Decoder, LSTM_Decoder\n",
    "from machine_learning.anomaly_detection import AutoEncoder, ED_Feedback"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Versuchsparameter\n",
    "normal = 0\n",
    "anomalies_percentage = 2\n",
    "dataset = 'energy'\n",
    "model_type = 'cnn-lstm-sa'\n",
    "\n",
    "# ML-Parameter\n",
    "batch_size = 32\n",
    "\n",
    "# automatisch generiert\n",
    "classes = [0,3] #list(range(num_anomaly+1))\n",
    "percentage = anomalies_percentage/100\n",
    "model_str = f'{dataset}-c{\"\".join(str(c)+\"-\" for c in classes)}p{anomalies_percentage}.hdf5'\n",
    "\n",
    "model_dict = {\n",
    "    'cnn': (CNN_Encoder, CNN_Decoder),\n",
    "    'cnn-opt': (CNN_Encoder, CNN_Decoder),\n",
    "    'cnn-vae': (CNN_Encoder, CNN_Decoder),\n",
    "    'cnn-mec': (CNN_Encoder, CNN_Decoder),\n",
    "    'wavenet': (WaveNet_Encoder, WaveNet_Decoder),\n",
    "    'wavenet-mec': (WaveNet_Encoder, WaveNet_Decoder),\n",
    "    'lstm': (LSTM_Encoder, LSTM_Decoder),\n",
    "    'cnn-lstm': (CNN_LSTM_Encoder, CNN_LSTM_Decoder),\n",
    "    'cnn-lstm-sa': (CNN_LSTM_SA_Encoder, CNN_LSTM_SA_Decoder),\n",
    "}\n",
    "\n",
    "# get the working directory of the ipykernel\n",
    "working_dir = os.getcwd()\n",
    "\n",
    "# define subdirectories\n",
    "data_path = os.path.join(working_dir, \"data\")\n",
    "dataset_path = os.path.join(data_path, dataset)\n",
    "trainingset_path = os.path.join(dataset_path, \"train\")\n",
    "testset_path = os.path.join(dataset_path, \"test\")\n",
    "\n",
    "models_path = os.path.join(working_dir, \"saved_models\")\n",
    "output_path = os.path.join(models_path, model_type)\n",
    "Path(output_path).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(model_str)\n",
    "print(trainingset_path)\n",
    "print(output_path)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def aursad(dataset='train', classes=classes):\n",
    "\n",
    "    path = None\n",
    "    if dataset.lower() == 'train':\n",
    "        path = trainingset_path\n",
    "    elif dataset.lower() == 'test':\n",
    "        path = testset_path\n",
    "    else:\n",
    "        raise ValueError(\"Invalid dataset specified\")\n",
    "    \n",
    "    x = np.load(os.path.join(path, \"x.npy\"))\n",
    "    y = np.load(os.path.join(path, \"y.npy\"))\n",
    "    \n",
    "    # extract the relevant classes only\n",
    "    mask = [i in classes for i in y]\n",
    "    x = x[mask,:,:]\n",
    "    y = y[mask]\n",
    "    \n",
    "    return x, y\n",
    "\n",
    "def unsupervised(percentage=percentage, batch_size=batch_size):\n",
    "    \n",
    "    x,y = aursad()\n",
    "        \n",
    "    num_normal_samples = np.sum(y == normal)\n",
    "    num_anomalies = int(num_normal_samples / (1-percentage) * percentage)\n",
    "\n",
    "    # extracts the first anomalous occurencies, until the desired percentage is achieved\n",
    "    anomaly_idx = y != normal\n",
    "    cum_anomaly_idx = np.cumsum(anomaly_idx)\n",
    "    anomaly_mask = cum_anomaly_idx <= num_anomalies\n",
    "\n",
    "    mask = np.logical_or(y == normal, anomaly_mask)\n",
    "\n",
    "    x = x[mask,::]\n",
    "    y = y[mask]\n",
    "    \n",
    "    total = x.shape[0]\n",
    "    n = (total // batch_size) * batch_size\n",
    "    \n",
    "    x = x[:n,:,:]\n",
    "    y = y[:n]\n",
    "       \n",
    "    return x, y\n",
    "\n",
    "def semi_supervised(batch_size=batch_size):\n",
    "    \n",
    "    return unsupervised(percentage=0, batch_size=batch_size)\n",
    "\n",
    "def supervised(dataset = \"train\", batch_size = batch_size):\n",
    "    \n",
    "    x,y = aursad(dataset=dataset)\n",
    "    \n",
    "    total = x.shape[0]\n",
    "    n = (total // batch_size) * batch_size\n",
    "    \n",
    "    x = x[:n,:,:]\n",
    "    y = y[:n]\n",
    "       \n",
    "    return x, y\n",
    "\n",
    "def holdout_validation(x, y, percentage=0.8, batch_size=batch_size):\n",
    "\n",
    "    split_idx = int((len(y) * percentage // batch_size) * batch_size)\n",
    "\n",
    "    x_train = x[:split_idx,::]\n",
    "    y_train = y[:split_idx]\n",
    "    x_val = x[split_idx:,::]\n",
    "    y_val = y[split_idx:]\n",
    "\n",
    "    return (x_train, y_train), (x_val, y_val)\n",
    "\n",
    "\n",
    "x_peek, _= aursad()\n",
    "sequence_length = x_peek.shape[1]\n",
    "num_features = x_peek.shape[2]\n",
    "\n",
    "print(f'Sequence length = {sequence_length}')\n",
    "print(f'Number of features = {num_features}')\n",
    "\n",
    "print(len(unsupervised(percentage=percentage)[0]))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "latent_dim = 6\n",
    "\n",
    "# load the training data\n",
    "train_x, _ = unsupervised()\n",
    "\n",
    "# build the model\n",
    "vaeAD = AutoEncoder(encoder=model_dict[model_type][0], decoder=model_dict[model_type][1],\n",
    "                  input_shape=train_x.shape, latent_dim=latent_dim, is_variational=False)\n",
    "\n",
    "vae = vaeAD.get_model()\n",
    "vae.summary()\n",
    "\n",
    "encoder = vaeAD.get_encoder()\n",
    "encoder.summary()\n",
    "\n",
    "decoder = vaeAD.get_decoder()\n",
    "decoder.summary()"
   ],
   "outputs": [],
   "metadata": {
    "scrolled": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# train the VAE on MNIST digits\n",
    "x, y = unsupervised()\n",
    "(x_train, _), (x_val, _) = holdout_validation(x, y)\n",
    "print(x_train.shape)\n",
    "\n",
    "callbacks = [\n",
    "        keras.callbacks.ModelCheckpoint(\n",
    "        monitor = 'val_loss',\n",
    "        filepath = os.path.join(output_path, model_str), \n",
    "        save_weights_only = False,\n",
    "        save_best_only = True),\n",
    "\n",
    "        keras.callbacks.EarlyStopping(monitor='val_loss', patience=8)\n",
    "        ]\n",
    "\n",
    "history = None\n",
    "training = False\n",
    "\n",
    "if training:\n",
    "    history = vae.fit(x_train, x_train, epochs=360, shuffle=True, batch_size=batch_size,\n",
    "        callbacks=callbacks, validation_data=(x_val, x_val))\n",
    "    with open(os.path.join(output_path, \"history.json\"), 'w') as file:\n",
    "            json.dump(history.history, file)\n",
    "else:\n",
    "    vae.load_weights(os.path.join(output_path, model_str))"
   ],
   "outputs": [],
   "metadata": {
    "scrolled": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def plot_history(history, metrics):\n",
    "\n",
    "    for metric in metrics:\n",
    "        data = history.history[metric]\n",
    "        plt.plot(data)\n",
    "    plt.legend(metrics)\n",
    "\n",
    "unsupervised_metrics = [\"loss\", \"val_loss\"]\n",
    "#plot_history(history, unsupervised_metrics)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def plot_reconstruction(sample=0, feature=0):\n",
    "\n",
    "    x = x_train[sample, :, feature]\n",
    "    x_hat = vae.predict(x_train)[sample, :, feature]\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(8,4))\n",
    "    time = np.linspace(0, 18, num=x.shape[0])\n",
    "    time_ticks = np.arange(0, 20, step=2)\n",
    "\n",
    "    ax.plot(time, x, label=r'$x$')\n",
    "    ax.plot(time, x_hat, label=r'$\\hat{x}$')\n",
    "\n",
    "    ax.set_xticks(time_ticks)\n",
    "    ax.set_xlabel(\"Zeit [s]\", fontsize=14)\n",
    "    ax.set_ylabel(\"normalisierter Gesamtstrom [-]\", fontsize=14)\n",
    "\n",
    "    ax.legend(fontsize=16)\n",
    "\n",
    "    fig.tight_layout()\n",
    "    #fig.savefig('figures/lstm-fails.pdf')\n",
    "\n",
    "plot_reconstruction(sample=3, feature=1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#keras.utils.plot_model(vae, show_shapes = True)"
   ],
   "outputs": [],
   "metadata": {
    "scrolled": true
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Aktives Lernen - Feedback für das Gesamtmodell\n",
    "Im unüberwachten Szenario ist der Verlust unverändert, im überwachten Szenario wird auf den Verlus der BinaryCrossEntropyLoss des Klassifikator aufaddiert"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "activeAD = ED_Feedback(vaeAD)\n",
    "#keras.utils.plot_model(activeAD.get_supervised(), show_shapes=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#keras.utils.plot_model(vae_al, show_shapes = True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def attention_visualizer():\n",
    "    \n",
    "    #(orig, _), (_, _) = aursad()\n",
    "    \n",
    "    attention_vis = vaeAD.encoderWrapper.attention_model()\n",
    "    \n",
    "    scores = attention_vis.predict(x_train[:batch_size,:,:])[0,:,:]\n",
    "    #scores = np.log(scores)\n",
    "    print(\"scores: \",scores.shape)\n",
    "    print(f'max: {np.max(scores)}, min: {np.min(scores)}')\n",
    "\n",
    "    #plt.imshow(scores, cmap='hot', interpolation='nearest')\n",
    "    #plt.show()\n",
    "    \n",
    "    n = scores.shape[0]\n",
    "    n = n//10\n",
    "    \n",
    "    print(np.sum(scores, axis=1))\n",
    "    \n",
    "    fig, (ax1, ax2) = plt.subplots(1,2,gridspec_kw={'width_ratios': [3, 1]}, figsize=(10,6))\n",
    "    im = ax1.imshow(scores)\n",
    "    \n",
    "    ax1.set_xticks(np.arange(10)*n)\n",
    "    ax1.set_yticks(np.arange(10)*n)\n",
    "\n",
    "    # Loop over data dimensions and create text annotations.\n",
    "    for i in range(10):\n",
    "        for j in range(10):\n",
    "            text = ax1.text(j*n, i*n, np.round(scores[i*n, j*n],2), ha=\"center\", va=\"center\", color=\"w\")\n",
    "\n",
    "    ax1.set_title(\"Attention heat map\")\n",
    "\n",
    "    clb = fig.colorbar(cm.ScalarMappable(norm=Normalize(vmin=np.min(scores), vmax=np.max(scores))))\n",
    "    \n",
    "    ax2.plot(x_train[0,:,0],-np.arange(0,len(x_train[0,:,0])))\n",
    "    ax2.set_ylim(-len(x_train[0,:,0]),0)\n",
    "    \n",
    "    fig.tight_layout()\n",
    "    plt.show()\n",
    "   \n",
    "attention_visualizer()"
   ],
   "outputs": [],
   "metadata": {
    "scrolled": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#assert False"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def color_from_label(labels):\n",
    "    l2c = {0:'tab:blue',\n",
    "           1:'tab:orange',\n",
    "           2:'tab:green',\n",
    "           3:'tab:red',\n",
    "           4:'tab:purple',\n",
    "          }\n",
    "    return [l2c[label] for label in labels]\n",
    "\n",
    "\n",
    "def latent_space_pca(dim=2, dataset='train'):\n",
    "\n",
    "    x, y = supervised(dataset=dataset)\n",
    "    pca = PCA(n_components=dim)\n",
    "\n",
    "    z = encoder.predict(x)\n",
    "    z_pca = pca.fit_transform(z)\n",
    "\n",
    "    if dim==3:\n",
    "        from mpl_toolkits import mplot3d\n",
    "        fig = plt.figure(figsize=(8,8))\n",
    "        ax = plt.axes(projection='3d')\n",
    "        ax.scatter3D(z_pca[:,0], z_pca[:,1], z_pca[:,2], c=color_from_label(y), alpha=0.5)\n",
    "\n",
    "    plt.figure(figsize=(15,15))\n",
    "    plt.scatter(z_pca[:,0], z_pca[:,1], c=color_from_label(y))\n",
    "\n",
    "def visualize_latent_space(show_size=False, dataset='test'):\n",
    "\n",
    "    x, y = supervised(dataset=dataset)\n",
    "\n",
    "    z = encoder.predict(x)\n",
    "    z_1 = z[:,0]\n",
    "    z_2 = z[:,1]\n",
    "\n",
    "    plt.figure(figsize=(15,15))\n",
    "    plt.scatter(z_1, z_2, c=color_from_label(y))\n",
    "\n",
    "def export_latent_space(dataset='train'):\n",
    "    x, y = supervised(dataset=dataset)\n",
    "\n",
    "    z = encoder.predict(x)\n",
    "    np.savetxt('dumps/latent_vae_z.csv', z)\n",
    "    np.savetxt('dumps/latent_vae_y.csv', y)\n",
    "\n",
    "#visualize_latent_space()\n",
    "latent_space_pca(dim=2)\n",
    "#export_latent_space()"
   ],
   "outputs": [],
   "metadata": {
    "scrolled": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def mahalanobis_anomaly_score_series(x_true, x_pred):\n",
    "    \n",
    "    a = np.zeros((x_true.shape[0], x_true.shape[1]))\n",
    "    for t in range(x_true.shape[1]):\n",
    "        \n",
    "        xt_true = x_true[:,t,:]\n",
    "        xt_pred = x_pred[:,t,:]\n",
    "        \n",
    "        error = np.abs(xt_true-xt_pred)\n",
    "    \n",
    "        mu = np.mean(error, axis=0)\n",
    "        #print(mu.shape)\n",
    "        \n",
    "        cov = np.cov(error, rowvar=False)\n",
    "        if cov.shape == ():\n",
    "            inv_cov = 1/cov\n",
    "        else:\n",
    "            inv_cov = np.linalg.inv(cov)\n",
    "        #print(cov.shape)\n",
    "        \n",
    "        for s in range(x_true.shape[0]):\n",
    "            e = error[s,:]\n",
    "\n",
    "            if inv_cov.shape == ():\n",
    "                a[s,t] = inv_cov * np.matmul((e-mu).T,(e-mu))\n",
    "            else:\n",
    "                a[s,t] = np.matmul(np.matmul((e-mu).T, inv_cov),(e-mu))\n",
    "        \n",
    "    return a\n",
    "\n",
    "def show_anomaly_score(sample=0, label=0):\n",
    "    \n",
    "    orig, y = supervised()    \n",
    "    x_pred = vae.predict(orig)\n",
    "\n",
    "    a = mahalanobis_anomaly_score_series(orig, x_pred)\n",
    "    \n",
    "    s = 300\n",
    "    while y[s] != label:\n",
    "        s+=1   \n",
    "            \n",
    "    num_plots = orig.shape[2]+1\n",
    "    fig, ax = plt.subplots(num_plots,1,figsize=(15,num_plots*3))\n",
    "        \n",
    "    for i, axi in enumerate(ax):\n",
    "            \n",
    "        if axi != ax[-1]:\n",
    "            axi.plot(orig[s,:,i])\n",
    "            axi.plot(x_pred[s,:,i])\n",
    "        else:\n",
    "            break\n",
    "        \n",
    "    ax[-1].plot(a[s,:], c=\"r\")\n",
    "    ax[-1].hlines(np.mean(a[s,:]), 0,400)\n",
    "    ax[-1].legend([\"mahalanobis\", str(np.mean(a[s,:]))])\n",
    "    plt.show()\n",
    "\n",
    "def visualize_unsupervised_AD(label=0):\n",
    "\n",
    "    x, y = supervised()    \n",
    "    x_hat = vae.predict(x)\n",
    "    \n",
    "    s = 300\n",
    "    while y[s] != label:\n",
    "        s+=1   \n",
    "\n",
    "    a = mahalanobis_anomaly_score_series(x, x_hat)\n",
    "    anomaly_score = np.mean(a[s,:])\n",
    "    anomaly_score = np.round(anomaly_score, decimals=2)\n",
    "\n",
    "    time = np.linspace(0, 18, num=x.shape[1])\n",
    "    time_ticks = np.arange(0, 20, step=2)\n",
    "            \n",
    "    num_plots = x.shape[2]+1\n",
    "    fig, ax = plt.subplots(num_plots,1,figsize=(15,num_plots*3))\n",
    "\n",
    "    def plot_series(ax, x, x_hat, ylabel):\n",
    "        ax.plot(time, x, label=r'$x$')\n",
    "        ax.plot(time, x_hat, label=r'$\\hat{x}$')\n",
    "\n",
    "        ax.set_xticks(time_ticks)\n",
    "        #ax.set_xlabel(\"normalisierte Zeit [-]\", fontsize=14)\n",
    "        ax.set_ylabel(ylabel, fontsize=14)\n",
    "\n",
    "\n",
    "    # Gesamtstrom\n",
    "    plot_series(ax[0], x[s,:,1], x_hat[s,:,1], \"normalisierter\\nGesamtstrom [-]\")\n",
    "    # Achse 4\n",
    "    plot_series(ax[1], x[s,:,2], x_hat[s,:,2], \"normalisierter\\nMotorstrom 4 [-]\")\n",
    "    # Achse 5\n",
    "    plot_series(ax[2], x[s,:,3], x_hat[s,:,3], \"normalisierter\\nMotorstrom 5 [-]\")\n",
    "    # Achse 5\n",
    "    plot_series(ax[3], x[s,:,0], x_hat[s,:,0], \"normalisierter\\nMotorstrom 6 [-]\")\n",
    "        \n",
    "    ax[-1].plot(time, a[s,:], c=\"k\", label='Mahalanobis-Distanz')\n",
    "    ax[-1].hlines(anomaly_score, 0, time[-1], color='tab:red', label=f'Anomaliescore={anomaly_score}')\n",
    "    ax[-1].set_xticks(time_ticks)\n",
    "    ax[-1].set_xlabel(\"Zeit [s]\", fontsize=14)\n",
    "    ax[-1].set_ylabel('Anomaliescore [-]', fontsize=14)\n",
    "    #ax[-1].legend([\"mahalanobis\", str(np.mean(a[s,:]))])\n",
    "\n",
    "    handles, labels = [(a + b) for a, b in zip(ax[0].get_legend_handles_labels(), ax[-1].get_legend_handles_labels())]\n",
    "    \n",
    "    fig.legend(handles, labels, loc='lower center', bbox_to_anchor = (0.5,-0.03), ncol=4, fontsize=14)\n",
    "    fig.tight_layout()\n",
    "    #fig.savefig('figures/unsupervised_anomaly.pdf', bbox_inches='tight')\n",
    "        \n",
    "#show_anomaly_score(label=0)\n",
    "visualize_unsupervised_AD(label=0)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def mse(x_true, x_pred):\n",
    "        \n",
    "        loss = np.mean((x_true-x_pred)**2, axis=1)\n",
    "        loss = np.mean(loss, axis=-1)\n",
    "        return loss\n",
    "    \n",
    "def mae(x_true, x_pred):\n",
    "    loss = np.mean(np.abs(x_true-x_pred), axis=1)\n",
    "    loss = np.mean(loss, axis=-1)\n",
    "    return loss\n",
    "\n",
    "def mahalanobis_anomaly_score(x_true, x_pred):\n",
    "    \"\"\"Reduces the mahalanobis anomaly score series to a scalar\"\"\"\n",
    "    series = mahalanobis_anomaly_score_series(x_true, x_pred)\n",
    "    return np.mean(series, axis=1)\n",
    "\n",
    "def quantile_threshold(p, loss_func=mae, return_loss=False):\n",
    "    \n",
    "    x, _ = supervised(dataset='train')\n",
    "    x_hat = vae.predict(x)\n",
    "\n",
    "    loss = loss_func(x, x_hat)\n",
    "    threshold = np.quantile(loss, p)\n",
    "\n",
    "    if return_loss:\n",
    "        return threshold, loss\n",
    "    else:\n",
    "        return threshold\n",
    "\n",
    "def anomaly_detection():\n",
    "    \n",
    "    x_test, y_test = supervised(dataset=\"test\")\n",
    "    x_pred = vae.predict(x_test)\n",
    "\n",
    "    anomaly_scores = np.log(mahalanobis_anomaly_score(x_test, x_pred))\n",
    "    threshold = np.log(quantile_threshold(0.865, loss_func=mahalanobis_anomaly_score))\n",
    "    threshold = np.round(threshold, decimals=2)\n",
    "\n",
    "    all_scores = list()\n",
    "    labels = [0,3]\n",
    "    label_names = ['Normalfälle', 'Anomalien']\n",
    "\n",
    "    for label in labels:\n",
    "        scores = anomaly_scores[y_test== label]\n",
    "        if len(scores) == 0:\n",
    "            all_scores.append([0])\n",
    "        else:\n",
    "            all_scores.append(scores)\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10,4))\n",
    "    ax.boxplot(all_scores, vert=False, patch_artist=False, labels=label_names, notch=True, widths=0.5, whis=1.55)\n",
    "\n",
    "    ax.set_xlim([0.5,3.5])\n",
    "    ax.set_xlabel(\"log(Anomaliescore) [-]\", fontsize=14)\n",
    "    ax.set_yticklabels(['Normalfälle', 'Anomalien'], fontsize=14, rotation=90, verticalalignment='center')\n",
    "    \n",
    "    ax.axvline(threshold, linestyle='--', color=\"tab:blue\", label=r'Schwellenwert: $log(\\tilde{s})=$'+str(threshold))\n",
    "    ax.axhline(1.5, color='k', linewidth=1)\n",
    "\n",
    "    ax.annotate('TN', (1.5,1.3), fontsize=14)\n",
    "    ax.annotate('FP', (1.75,1.3), fontsize=14)\n",
    "    ax.annotate('FN', (1.5,1.55), fontsize=14)\n",
    "    ax.annotate('TP', (1.75,1.55), fontsize=14)\n",
    "\n",
    "    # tn = matplotlib.patches.Rectangle((0.5, 0.5), threshold-0.5, 1, color='tab:green', alpha=0.1)\n",
    "    # tp = matplotlib.patches.Rectangle((threshold, 1.5), 3.5-threshold, 1, color='tab:green', alpha=0.1)\n",
    "    # fp = matplotlib.patches.Rectangle((0.5, 1.5), threshold-0.5, 1, color='tab:red', alpha=0.1)\n",
    "    # fn = matplotlib.patches.Rectangle((threshold, 0.5), 3.5-threshold, 1, color='tab:red', alpha=0.1)\n",
    "    # ax.add_patch(tn)\n",
    "    # ax.add_patch(tp)\n",
    "    # ax.add_patch(fp)\n",
    "    # ax.add_patch(fn)\n",
    "    ax.legend(fontsize=13)#, loc='lower right', bbox_to_anchor=(1,-0.25))\n",
    "\n",
    "    #fig.savefig('figures/unsupervised_ad.pdf', bbox_inches='tight')\n",
    "\n",
    "#my_vae = load_vae(model_str)\n",
    "anomaly_detection()"
   ],
   "outputs": [],
   "metadata": {
    "scrolled": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def confusion_matrix(actual, predicted):\n",
    "    \"\"\"Computes the elements of a confusion matrix: tp, fp, fn, tn\n",
    "    \"\"\"\n",
    "\n",
    "    # positiv = anomalous, negative = normal\n",
    "    tp = np.sum(np.logical_and(actual == 1, predicted == 1))\n",
    "    tn = np.sum(np.logical_and(actual == 0, predicted == 0))\n",
    "    fp = np.sum(np.logical_and(actual == 0, predicted == 1))\n",
    "    fn = np.sum(np.logical_and(actual == 1, predicted == 0))\n",
    "    \n",
    "    return tp, fp, fn, tn\n",
    "\n",
    "def accuracy(tp, fp, fn, tn):  \n",
    "    return (tp+tn) / (tp+fp+fn+tn)\n",
    "\n",
    "def precision(tp, fp, fn, tn):\n",
    "    return tp/(tp+fp)\n",
    "\n",
    "def recall(tp, fp, fn, tn):\n",
    "    return tp/(tp+fn)\n",
    "\n",
    "def f_score(tp, fp, fn, tn):\n",
    "    p = precision(tp, fp, fn, tn)\n",
    "    r = recall(tp, fp, fn, tn)\n",
    "    return 2*p*r/(p+r)\n",
    "\n",
    "def mcc_score(tp, fp, fn, tn):\n",
    "    return (tp*tn-fp*fn)/np.sqrt((tp+fp)*(tp+fn)*(tn+fp)*(tn+fn))\n",
    "\n",
    "def baseline_ad(model = \"unsupervised\", loss_func=mae):\n",
    "    \n",
    "    ##x_train, _ = unsupervised(percentage=0.125)\n",
    "    #x_train, _ = supervised(dataset=\"train\")\n",
    "    #    \n",
    "    #x_pred = vae.predict(x_train)\n",
    "    #training_loss = loss_func(x_train, x_pred)    \n",
    "    #    \n",
    "    #critical_anomaly_score = np.quantile(training_loss, 0.865)\n",
    "\n",
    "    critical_anomaly_score = quantile_threshold(0.865, loss_func)\n",
    "    \n",
    "    print(f'Critical Anomaly Score = {critical_anomaly_score}')\n",
    "    \n",
    "    x_test, y_test = supervised(dataset=\"test\")\n",
    "    \n",
    "    for i in range(5):\n",
    "        count = np.sum(y_test==i)\n",
    "        print(f'Anzahl {i}: {count}')\n",
    "    \n",
    "    x_test_pred = vae.predict(x_test)\n",
    "    test_loss = loss_func(x_test, x_test_pred)    \n",
    "    \n",
    "    predicted = test_loss > critical_anomaly_score\n",
    "    actual = y_test != 0\n",
    "    \n",
    "    tp, fp, fn, tn = confusion_matrix(actual, predicted)\n",
    "    n = len(y_test)\n",
    "    \n",
    "    print(f'TP = {tp}, TN = {tn}, FP = {fp}, FN = {fn}, N = {n}')\n",
    "    \n",
    "    acc = accuracy(tp, fp, fn, tn)\n",
    "    prec = precision(tp, fp, fn, tn)\n",
    "    rec = recall(tp, fp, fn, tn)\n",
    "    f1 = f_score(tp, fp, fn, tn)\n",
    "    mcc = mcc_score(tp, fp, fn, tn)\n",
    "    print(f'Accuracy = {acc}, F1 = {f1}, precision = {prec}, recall = {rec}, mcc = {mcc}')\n",
    "    \n",
    "    \n",
    "baseline_ad(loss_func=mahalanobis_anomaly_score)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def f1_opt():\n",
    "\n",
    "    x, y = supervised(dataset='test')\n",
    "    y = y != 0\n",
    "    x_hat = vae.predict(x)\n",
    "    score = mahalanobis_anomaly_score(x, x_hat)\n",
    "\n",
    "    minScore = np.min(score)\n",
    "    maxScore = np.max(score)\n",
    "    scores = np.arange(minScore, maxScore, 0.01)\n",
    "\n",
    "    opt = -1\n",
    "    threshold = None\n",
    "    for s in scores:\n",
    "        y_pred = score > s\n",
    "        tp, fp, fn, tn = confusion_matrix(y, y_pred)\n",
    "        new = f_score(tp, fp, fn, tn)\n",
    "        if new > opt:\n",
    "            opt = new\n",
    "            threshold=s\n",
    "    \n",
    "    # report\n",
    "    print(f'opt. score (query dataset): {opt} @ opt. threshold: {threshold}')\n",
    "\n",
    "f1_opt()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "def roc_curve():\n",
    "\n",
    "    x, y_true = supervised(dataset=\"test\")\n",
    "    x_hat = vae.predict(x)\n",
    "\n",
    "    y_true = y_true != 0\n",
    "    y_score = mahalanobis_anomaly_score(x, x_hat)\n",
    "    threshold = quantile_threshold(0.865, loss_func=mahalanobis_anomaly_score)\n",
    "\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(y_true, y_score)\n",
    "    roc_auc = metrics.auc(fpr, tpr)\n",
    "\n",
    "    print(\"\".join(str(t)+\",\" for t in tpr))\n",
    "    print(\"\".join(str(f)+\",\" for f in fpr))\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(8,8))\n",
    "    ax.plot(fpr, tpr, label=f'ROC AUC = {roc_auc}')\n",
    "    ax.plot([0,1],[0,1], '--')\n",
    "\n",
    "    ax.legend()\n",
    "\n",
    "    gmeans = np.sqrt(tpr * (1-fpr))\n",
    "    ix = np.argmax(gmeans)\n",
    "\n",
    "    ax.plot(fpr[ix], tpr[ix], 'x')\n",
    "    minx = np.argmin(np.abs(thresholds-threshold))\n",
    "    ax.plot(fpr[minx], tpr[minx], 'x')\n",
    "    print(f'Optimal threshold = {thresholds[ix]}')\n",
    "    print(f'p-quantile threshold = {threshold}')\n",
    "\n",
    "    predicted = y_score > thresholds[ix]\n",
    "    tp, fp, fn, tn = confusion_matrix(y_true, predicted)\n",
    "    f1 = f_score(tp, fp, fn, tn)\n",
    "    \n",
    "    print(f'F1 = {f1}')\n",
    "    print(f'Max AUC = {np.max(gmeans**2)}')\n",
    "    print(f'Max AUC = {tpr[minx] * (1-fpr[minx])}')\n",
    "    \n",
    "#roc_curve()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Query Strategy"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def naive_query(budget, return_idx=False):\n",
    "    # query the entire, non-restricted \n",
    "    x, y = unsupervised(percentage=0.99)\n",
    "    y = y != 0\n",
    "\n",
    "    x = x[:budget,:,:]  \n",
    "    y = y[:budget]   \n",
    "    \n",
    "    if return_idx:\n",
    "        return (x,y), np.arange(budget)\n",
    "    else:\n",
    "        return x, y\n",
    "    \n",
    "def edgecase_query(budget, return_idx=False):\n",
    "    # query the entire, non-restricted \n",
    "    x, y = unsupervised(percentage=0.99)\n",
    "    y = y != 0\n",
    "\n",
    "    threshold, score = quantile_threshold(0.865,\n",
    "                            loss_func=mahalanobis_anomaly_score, return_loss=True)\n",
    "    #print(f'Threshold anomaly score: {threshold}')\n",
    "        \n",
    "    edge_cases = np.abs(score - threshold)    \n",
    "    idx = np.argsort(edge_cases)\n",
    "    idx = idx[:budget]\n",
    "        \n",
    "    if return_idx:\n",
    "        return (x[idx,::], y[idx]), idx\n",
    "    else:\n",
    "        return x[idx,::], y[idx]\n",
    "    \n",
    "def most_anomalous_query(budget, return_idx=False):\n",
    "    # query the entire, non-restricted \n",
    "    x, y = unsupervised(percentage=0.99)\n",
    "    y = y != 0\n",
    "    \n",
    "    x_hat = vae.predict(x)\n",
    "    score = mahalanobis_anomaly_score(x, x_hat)\n",
    "\n",
    "    idx = np.argsort(score)\n",
    "    idx = idx[-budget:]\n",
    "        \n",
    "    if return_idx:\n",
    "        return (x[idx,::], y[idx]), idx\n",
    "    else:\n",
    "        return x[idx,::], y[idx]\n",
    "\n",
    "def normal_query(budget, return_idx=False):\n",
    "    # query the entire, non-restricted \n",
    "    x, y = unsupervised(percentage=0.99)\n",
    "    y = y != 0\n",
    "    \n",
    "    x_hat = vae.predict(x)\n",
    "    score = mahalanobis_anomaly_score(x, x_hat)\n",
    "\n",
    "    idx = np.argsort(score)\n",
    "    idx = idx[:budget]\n",
    "        \n",
    "    if return_idx:\n",
    "        return (x[idx,::], y[idx]), idx\n",
    "    else:\n",
    "        return x[idx,::], y[idx]\n",
    "\n",
    "\n",
    "def clustering_query(budget, plot=False, return_idx=False):\n",
    "    # query the entire, non-restricted \n",
    "    x, y = unsupervised(percentage=0.99)\n",
    "    y = y != 0\n",
    "      \n",
    "    def dbscan_clustering(z, sample_weight=None):\n",
    "        y = DBSCAN(min_samples=9, eps=0.1).fit_predict(z, sample_weight=sample_weight)\n",
    "        return y\n",
    "    \n",
    "    def kmeans_clustering(z, sample_weight=None):\n",
    "        kmeans = KMeans(n_clusters=10, random_state=0).fit(z, sample_weight=sample_weight)\n",
    "        return kmeans.predict(z)\n",
    "    \n",
    "\n",
    "    threshold, score = quantile_threshold(0.865,\n",
    "                            loss_func=mahalanobis_anomaly_score, return_loss=True)\n",
    "\n",
    "    sample_weight = score > threshold\n",
    "    sample_weight = sample_weight * 3 + 1\n",
    "\n",
    "    pca = PCA(n_components=2)\n",
    "    z = encoder.predict(x)\n",
    "    z_pca = pca.fit_transform(z)\n",
    "\n",
    "    y_cluster = dbscan_clustering(z_pca, sample_weight=sample_weight)\n",
    "    \n",
    "    import pandas as pd\n",
    "    y_series = pd.Series(y_cluster)\n",
    "\n",
    "    #print(y_series)\n",
    "    grouped = y_series.groupby(y_series).count().reset_index()\n",
    "    #print(grouped.columns)\n",
    "    grouped=grouped.sort_values(0)\n",
    "    #print(grouped)\n",
    "\n",
    "    counts = grouped[0]\n",
    "    #print(counts)\n",
    "\n",
    "    idx = list()\n",
    "    for j, (i, _) in enumerate(counts.iteritems()):\n",
    "        \n",
    "        remaining = budget - len(idx)\n",
    "        samples_per_label = int(np.ceil(remaining/(len(counts)-j)))\n",
    "        \n",
    "        pos = np.arange(len(y_series))\n",
    "        new_idx = pos[y_series==(i-1)][:samples_per_label]\n",
    "        idx.extend(new_idx)\n",
    "        #print(i,_,len(new_idx))\n",
    "\n",
    "    if plot:\n",
    "        print(np.unique(y_cluster))\n",
    "        fig = plt.figure(figsize=(12,12))\n",
    "        plt.scatter(z_pca[:,0], z_pca[:,1],c=y_cluster, s=100, cmap=cm.tab20)\n",
    "        z_neg = z_pca[y_cluster<0,:]\n",
    "        plt.scatter(z_neg[:,0], z_neg[:,1],c='k', s=100)\n",
    "        plt.scatter(z_pca[idx,0], z_pca[idx,1], edgecolors='r', s=160, marker='o', facecolors='none')\n",
    "\n",
    "    if return_idx:\n",
    "        return (x[idx,::], y[idx]), idx\n",
    "    else:\n",
    "        return x[idx,::], y[idx]\n",
    "    \n",
    "#_,_ = clustering_query(128, plot=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## E2: Vergleich der Abfragestrategien\n",
    "\n",
    "Für die Ergebnisse des CNN-LSTM Modell werden die verschiedenen Abfragestrategien verglichen\n",
    "\n",
    "Darstellungen:\n",
    "* a - Quantitativ: Wie viele Beispiele der einzelnen Klassen werden ausgewählt?\n",
    "* b - Qualitativ: Welche Beispiele (Lage im latenten Raum) der einzelnen Klassen werden ausgewählt?\n",
    "* c - Wirkung: Welche Leistungsfähigkeit des überwachten Modells ist damit erreichbar?"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def e2a():\n",
    "    budget = 256\n",
    "\n",
    "    queries = [naive_query, edgecase_query, most_anomalous_query, normal_query, clustering_query]\n",
    "    for query in queries:\n",
    "        qx, qy = query(budget)\n",
    "\n",
    "        num_normal = np.sum(qy == 0)\n",
    "        num_anomalies = np.sum(qy != 0)\n",
    "        print(f\"Query: {query.__name__}, Normal: {num_normal}, anomalies {num_anomalies}\")\n",
    "\n",
    "#e2a()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "from matplotlib.gridspec import GridSpec\n",
    "from matplotlib.colors import Normalize\n",
    "import pandas as pd\n",
    "\n",
    "def e2b():\n",
    "    # query the entire, non-restricted \n",
    "    x, y = unsupervised(percentage=0.99)\n",
    "    y = y != 0\n",
    "    \n",
    "    pca = PCA(n_components=2)\n",
    "    z = encoder.predict(x)\n",
    "    z_pca = pca.fit_transform(z)\n",
    "\n",
    "    x_hat = vae.predict(x)\n",
    "    threshold, score = quantile_threshold(0.865,\n",
    "                            loss_func=mahalanobis_anomaly_score, return_loss=True)\n",
    "    \n",
    "    # make the scores logarithmic for a enhanced depiction\n",
    "    threshold, score = np.log(threshold), np.log(score)\n",
    "\n",
    "    (_,_), idx = clustering_query(budget=64, return_idx=True)\n",
    "\n",
    "    fig= plt.figure(figsize=(12,8))\n",
    "    grid_spec = fig.add_gridspec(ncols=2, nrows=2, height_ratios=[8,1], width_ratios=[9.8, 0.2])\n",
    "\n",
    "    ax1 = fig.add_subplot(grid_spec[0,0])\n",
    "    ax2 = fig.add_subplot(grid_spec[1,:])\n",
    "    axcb = fig.add_subplot(grid_spec[0,1])\n",
    "\n",
    "    ax1.scatter(z_pca[:,0], z_pca[:,1], c=score, cmap=cm.RdYlGn_r)\n",
    "    ax1.scatter(z_pca[idx,0], z_pca[idx,1], edgecolors='r', marker='o', facecolors='none', s=160)\n",
    "    ax1.set_xlabel(r'$z_1$ [-]', fontsize=14)\n",
    "    ax1.set_ylabel(r'$z_2$ [-]', fontsize=14)\n",
    "\n",
    "    clb = fig.colorbar(cm.ScalarMappable(cmap=cm.RdYlGn_r, norm=Normalize(vmin=np.min(score), vmax=np.max(score))), cax=axcb)\n",
    "    clb.set_label('log(Anomaliescore)', fontsize=14)\n",
    "    ax2.scatter(score, np.ones(len(score)), s=20, c='k', label='Beispiel')\n",
    "    ax2.set_yticks([])\n",
    "    ax2.scatter(score[idx], np.ones(len(idx)), edgecolors='r', marker='o', facecolors='none', s=100, label='abgefragt')\n",
    "    ax2.vlines(threshold, 0.95, 1.05, linestyle=\"--\", color=\"tab:orange\", label=\"Schwellenwert\")\n",
    "    ax2.set_xlabel(\"log(Anomaliescore)\", fontsize=14)\n",
    "    ax2.legend(ncol=3, loc='upper center', bbox_to_anchor=(0.5,-0.65), fontsize=12)\n",
    "    fig.tight_layout()\n",
    "\n",
    "    #fig.savefig('figures/clustering_query.pdf')\n",
    "\n",
    "#e2b()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def active_learning(x_query, y_query, strategy='none', epochs=180):\n",
    "    \n",
    "    budget = len(y_query)\n",
    "    model_file = os.path.join(output_path, f'b{budget}-s{strategy}-{model_str}')\n",
    "    y_query = keras.utils.to_categorical(y_query, num_classes=2)\n",
    "\n",
    "    callbacks = [\n",
    "        keras.callbacks.ModelCheckpoint(\n",
    "            monitor = 'loss',\n",
    "            filepath = model_file, \n",
    "            save_weights_only = False,\n",
    "            save_best_only = True),\n",
    "        keras.callbacks.EarlyStopping(monitor='loss', patience=2)\n",
    "        ]\n",
    "\n",
    "    history = activeAD.get_supervised().fit(x_query, y_query, shuffle = True, \n",
    "                epochs=epochs, batch_size=64, callbacks=callbacks)\n",
    "    \n",
    "    plot_history(history, ['loss'])\n",
    "    with open(os.path.join(output_path, \"active-history-1.json\"), 'w') as file:\n",
    "            json.dump(history.history, file)\n",
    "\n",
    "    x_test, y_test = supervised(dataset = \"test\")\n",
    "    y_test = y_test != 0\n",
    "    y_test = keras.utils.to_categorical(y_test, num_classes=2)\n",
    "    \n",
    "    predicted = activeAD.get_supervised().predict(x_test)\n",
    "    predicted = np.argmax(predicted, axis=-1)\n",
    "    actual = np.argmax(y_test, axis=-1)\n",
    "\n",
    "    tp, fp, fn, tn = confusion_matrix(actual, predicted)\n",
    "\n",
    "    tp, fp, fn, tn = confusion_matrix(actual, predicted)\n",
    "    acc = accuracy(tp, fp, fn, tn)\n",
    "    prec = precision(tp, fp, fn, tn)\n",
    "    rec = recall(tp, fp, fn, tn)\n",
    "    f1 = f_score(tp, fp, fn, tn)\n",
    "    mcc = mcc_score(tp, fp, fn, tn)\n",
    "        \n",
    "    print(f'TP = {tp}, TN = {tn}, FP = {fp}, FN = {fn}')\n",
    "    print(f'Accuracy = {acc}, F1 = {f1}, precision = {prec}, recall = {rec}, mcc = {mcc}')\n",
    "\n",
    "def e2c():\n",
    "    qx1,qy1 = normal_query(budget=128)\n",
    "    qx2,qy2 = most_anomalous_query(budget=256)\n",
    "\n",
    "    qx = np.concatenate((qx1, qx2), axis=0)\n",
    "    qy = np.concatenate((qy1, qy2), axis=0)\n",
    "    \n",
    "    active_learning(qx, qy, strategy='M')\n",
    "\n",
    "#e2c()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## E3: Vergleich der Architekturen\n",
    "Für die Abfragestrategie `most_anomalous_query` ergänzt um die `normal_query` werden die verschiedenen Architekturen bei B=256 verglichen."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "qx1,qy1 = normal_query(budget=128)\n",
    "qx2,qy2 = most_anomalous_query(budget=256)\n",
    "qx = np.concatenate((qx1, qx2), axis=0)\n",
    "qy = np.concatenate((qy1, qy2), axis=0)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "active_learning(qx, qy, strategy='MN')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "assert False"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def e3():\n",
    "    qx1,qy1 = normal_query(budget=128)\n",
    "    qx2,qy2 = most_anomalous_query(budget=256)\n",
    "\n",
    "    qx = np.concatenate((qx1, qx2), axis=0)\n",
    "    qy = np.concatenate((qy1, qy2), axis=0)\n",
    "    \n",
    "    active_learning(qx, qy, strategy='MN')\n",
    "#e3()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Restrukturierung des latenten Raums"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def z_shift():\n",
    "    x, y = supervised(dataset='train')\n",
    "    z_before = encoder.predict(x)\n",
    "\n",
    "    qx1,qy1 = normal_query(budget=128)\n",
    "    qx2,qy2 = most_anomalous_query(budget=256)\n",
    "\n",
    "    qx = np.concatenate((qx1, qx2), axis=0)\n",
    "    qy = np.concatenate((qy1, qy2), axis=0)\n",
    "    \n",
    "    active_learning(qx, qy, strategy='zshift')\n",
    "\n",
    "    z_after = encoder.predict(x)\n",
    "\n",
    "    return z_before, z_after, y\n",
    "\n",
    "z_b, z_a, groundtruth = z_shift()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def plot_z_shift(z_b, z_a, y):\n",
    "    fig, axs = plt.subplots(ncols=2, figsize=(12,6))\n",
    "\n",
    "    def single_view(ax, z, y, text):\n",
    "\n",
    "        pca = PCA(n_components=2)\n",
    "        z_pca = pca.fit_transform(z)\n",
    "\n",
    "        label_from_y = {0: 'Normalfälle', 3: \"Anomalien\"}\n",
    "\n",
    "        for l in np.unique(y):\n",
    "            idx = y==l\n",
    "            ax.scatter(z_pca[idx,0], z_pca[idx,1], s=20, c=color_from_label(y[idx]), alpha=0.6, label=label_from_y[l])\n",
    "        ax.set_xlabel(r'$z_1$ [-]', fontsize=14)\n",
    "        ax.set_ylabel(r'$z_2$ [-]', fontsize=14)\n",
    "        ax.text(0.5, 1.05, text, transform=ax.transAxes, horizontalalignment='center', size=16)\n",
    "\n",
    "    single_view(axs[0], z_b, y, \"vor aktivem Lernen\")\n",
    "    single_view(axs[1], z_a, y, \"nach aktivem Lernen\")\n",
    "    handles, labels = axs[0].get_legend_handles_labels()\n",
    "    fig.legend(handles, labels, ncol=2, loc='lower center', bbox_to_anchor=(0.5, -0.01), fontsize=14)\n",
    "\n",
    "    fig.tight_layout()\n",
    "\n",
    "    fig.savefig('figures/zshift.pdf')\n",
    "\n",
    "plot_z_shift(z_b, z_a, groundtruth)\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## E4 Validierung\n",
    "\n",
    "Hyperparameter Optimierung des UAI"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def active_learning_validation(x_query, y_query, x_valid, y_valid):\n",
    "\n",
    "    callbacks = [\n",
    "        keras.callbacks.EarlyStopping(monitor='loss', patience=8)\n",
    "        ]\n",
    "\n",
    "    y_query = keras.utils.to_categorical(y_query, num_classes=2)\n",
    "    y_valid = keras.utils.to_categorical(y_valid, num_classes=2)\n",
    "\n",
    "    history = activeAD.get_supervised().fit(x_query, y_query, shuffle = True, \n",
    "                epochs=180, batch_size=32, callbacks=callbacks)#, validation_data=(x_valid, y_valid))\n",
    "    \n",
    "    plot_history(history, ['loss'])\n",
    "    \n",
    "    predicted = activeAD.get_supervised().predict(x_valid)\n",
    "    predicted = np.argmax(predicted, axis=-1)\n",
    "    actual = np.argmax(y_valid, axis=-1)\n",
    "\n",
    "    tp, fp, fn, tn = confusion_matrix(actual, predicted)\n",
    "\n",
    "    tp, fp, fn, tn = confusion_matrix(actual, predicted)\n",
    "    acc = accuracy(tp, fp, fn, tn)\n",
    "    prec = precision(tp, fp, fn, tn)\n",
    "    rec = recall(tp, fp, fn, tn)\n",
    "    f1 = f_score(tp, fp, fn, tn)\n",
    "    mcc = mcc_score(tp, fp, fn, tn)\n",
    "        \n",
    "    print(f'TP = {tp}, TN = {tn}, FP = {fp}, FN = {fn}')\n",
    "    print(f'Accuracy = {acc}, F1 = {f1}, precision = {prec}, recall = {rec}, mcc = {mcc}')\n",
    "\n",
    "def e4():\n",
    "    qx1,qy1 = normal_query(budget=128)\n",
    "    qx2,qy2 = most_anomalous_query(budget=256)\n",
    "\n",
    "    qx = np.concatenate((qx1, qx2), axis=0)\n",
    "    qy = np.concatenate((qy1, qy2), axis=0)\n",
    "\n",
    "    def unison_shuffled_copies(x, y):\n",
    "        assert x.shape[0] == y.shape[0]\n",
    "        rng = np.random.default_rng(0)    \n",
    "        p = rng.permutation(x.shape[0])\n",
    "        return x[p], y[p]\n",
    "\n",
    "    qx, qy = unison_shuffled_copies(qx, qy)\n",
    "\n",
    "    (x_train, y_train), (x_valid, y_valid) = holdout_validation(qx,qy, percentage=0.75)\n",
    "\n",
    "    print(x_train.shape, x_valid.shape)\n",
    "    \n",
    "    active_learning_validation(x_train, y_train, x_valid, y_valid)\n",
    "\n",
    "#e5()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Abschlusstraining für optimierte Parameter\n",
    "def e4b():\n",
    "    qx1,qy1 = normal_query(budget=128)\n",
    "    qx2,qy2 = most_anomalous_query(budget=256)\n",
    "\n",
    "    qx = np.concatenate((qx1, qx2), axis=0)\n",
    "    qy = np.concatenate((qy1, qy2), axis=0)\n",
    "    \n",
    "    active_learning(qx, qy, strategy='MN')\n",
    "    \n",
    "#e4b()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data Efficiency\n",
    "Evaluate the CNN-AE on energy data using the most anomalous query with different different budgets: [64, 128, 192, 256, 384, 512, 1024]"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def e5(budget=256):\n",
    "    normal_budget = budget//2\n",
    "    if budget > 512:\n",
    "        normal_budget = 64\n",
    "    qx1,qy1 = normal_query(budget=normal_budget)\n",
    "    qx2,qy2 = most_anomalous_query(budget=budget)\n",
    "\n",
    "    qx = np.concatenate((qx1, qx2), axis=0)\n",
    "    qy = np.concatenate((qy1, qy2), axis=0)\n",
    "\n",
    "    print(f'Normal: {np.sum(qy==0)-normal_budget}, Anomalies: {np.sum(qy!=0)}')\n",
    "\n",
    "    x_test, y_test = supervised(dataset='test')\n",
    "    y_test = y_test != 0\n",
    "    \n",
    "    active_learning_validation(qx, qy, x_test, y_test)\n",
    "#e5(budget=1024)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## EX: Feedback zur F-Score Optimierune\n",
    "\n",
    "Anmerkung: in finaler Version NICHT durchgeführt\n",
    "\n",
    "Vorteile F-Score:\n",
    "* einfach zu berechnen\n",
    "* gebräuchig für ML\n",
    "* Wertebereich [0,1]\n",
    "* Goodfellow, p. 423f\n",
    "* Hand, David\n",
    "\n",
    "Vorteile MCC:\n",
    "* unabhängig, welche Klasse als positiv / negativ deklariert wird\n",
    "* basiert direkt auf der Konfusionsmatirx, unabhängig von Imbalance"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "def eX():\n",
    "    xq, yq = naive_query(budget=256) \n",
    "    xq_hat = vae.predict(xq)\n",
    "    score = mahalanobis_anomaly_score(xq, xq_hat)\n",
    "\n",
    "    minScore = np.min(score)\n",
    "    maxScore = np.max(score)\n",
    "    scores = np.arange(minScore, maxScore, 0.01)\n",
    "\n",
    "    opt = -1\n",
    "    threshold = None\n",
    "    for s in scores:\n",
    "        y_pred = score > s\n",
    "        tp, fp, fn, tn = confusion_matrix(yq, y_pred)\n",
    "        new = f_score(tp, fp, fn, tn)\n",
    "        if new > opt:\n",
    "            opt = new\n",
    "            threshold=s\n",
    "    \n",
    "    # report\n",
    "    print(f'opt. score (query dataset): {opt} @ opt. threshold: {threshold}')\n",
    "\n",
    "    # final evaluation on test data\n",
    "    x_test, y_test = supervised(dataset='test')\n",
    "    actual = y_test != 0\n",
    "\n",
    "    x_test_hat = vae.predict(x_test)\n",
    "    test_score = mahalanobis_anomaly_score(x_test, x_test_hat)\n",
    "\n",
    "    predicted = test_score > threshold\n",
    "\n",
    "    tp, fp, fn, tn = confusion_matrix(actual, predicted)\n",
    "    acc = accuracy(tp, fp, fn, tn)\n",
    "    prec = precision(tp, fp, fn, tn)\n",
    "    rec = recall(tp, fp, fn, tn)\n",
    "    f1 = f_score(tp, fp, fn, tn)\n",
    "    mcc = mcc_score(tp, fp, fn, tn)\n",
    "        \n",
    "    print(f'TP = {tp}, TN = {tn}, FP = {fp}, FN = {fn}')\n",
    "    print(f'Accuracy = {acc}, F1 = {f1}, precision = {prec}, recall = {rec}, mcc = {mcc}')\n",
    "\n",
    "#e3()"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "96ba8bdb9944f4077aea7f2f55699da4fa58fb876a32e9ef6c1a749faec584e6"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit ('m1tf': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}